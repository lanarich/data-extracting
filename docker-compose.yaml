# docker-compose.yml
# Определяет сервисы PostgreSQL, Qdrant, Infinity Embedding Server, Backend и Open WebUI
services:
  # postgres:
  #   image: postgres:14-alpine
  #   container_name: ai_tutor_postgres
  #   environment:
  #     POSTGRES_USER: ${POSTGRES_USER:-developer}
  #     POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-lanarich} # ЗАМЕНИТЕ ИЛИ ИСПОЛЬЗУЙТЕ .ENV!
  #     POSTGRES_DB: ${POSTGRES_DB:-ai_tutor_db}
  #   ports:
  #     - "${POSTGRES_PORT:-5432}:5432"
  #   volumes:
  #     - postgres_data:/var/lib/postgresql/data
  #   networks:
  #     - ai_tutor_net
  #   healthcheck: # Добавлена проверка работоспособности PostgreSQL
  #     test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-developer} -d ${POSTGRES_DB:-ai_tutor_db}"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #   restart: unless-stopped

  # qdrant:
  #   image: qdrant/qdrant:v1.14.0 # Рекомендуется использовать последнюю стабильную версию
  #   container_name: ai_tutor_qdrant
  #   ports:
  #     - "${QDRANT_HTTP_PORT:-6333}:6333" # HTTP/REST API
  #     - "${QDRANT_GRPC_PORT:-6334}:6334" # gRPC API
  #   volumes:
  #     - ./.qdrant_storage:/qdrant/storage # Используем bind mount
  #   networks:
  #     - ai_tutor_net
  #   restart: unless-stopped

  infinity:
    image: michaelf34/infinity:0.0.76 # Уточните последнюю версию, если нужно
    container_name: infinity_rag
    restart: unless-stopped
    ports:
      - "7997:7997" # API сервиса Infinity
    volumes:
      - ./.infinity_cache:/app/.cache/huggingface # Кэш Hugging Face
    command:
      [
        "v2",
        "--model-id", "sergeyzh/BERTA", # Модель эмбеддингов
        "--device", "cuda", # Использовать GPU
        "--batch-size", "50",
      ]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - ai_tutor_net

  # ai_tutor_backend: # Наш FastAPI бэкенд
  #   container_name: ai_tutor_backend
  #   build:
  #     context: . # Собираем образ из текущей директории (где лежит Dockerfile)
  #     dockerfile: Dockerfile # Имя Dockerfile
  #   ports:
  #     - "${APP_PORT:-8000}:8000" # Проброс порта FastAPI
  #   volumes:
  #     - .:/app # Монтируем код приложения внутрь контейнера для разработки
  #   environment:
  #     # Передаем переменные окружения, необходимые для работы приложения
  #     # Они будут прочитаны Hydra через oc.env
  #     - POSTGRES_USER=${POSTGRES_USER:-developer}
  #     - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-lanarich} # ЗАМЕНИТЕ ИЛИ ИСПОЛЬЗУЙТЕ .ENV!
  #     - POSTGRES_DB=${POSTGRES_DB:-ai_tutor_db}
  #     - POSTGRES_HOST=postgres # Используем имя сервиса postgres для подключения из бэкенда
  #     - POSTGRES_PORT=5432
  #     - QDRANT_HOST=qdrant # Используем имя сервиса qdrant
  #     - QDRANT_HTTP_PORT=6333
  #     - QDRANT_GRPC_PORT=6334
  #     - EMBEDDING_API_BASE=http://infinity_rag:7997
  #     - COGITO_API_BASE=${COGITO_API_BASE}
  #     - COGITO_API_KEY=${COGITO_API_KEY}
  #     - LANGFUSE_ENABLED=${LANGFUSE_ENABLED:-true}
  #     - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY}
  #     - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY}
  #     - LANGFUSE_HOST=${LANGFUSE_HOST:-https://cloud.langfuse.com}
  #     # Указываем Python не буферизировать вывод, чтобы логи сразу появлялись
  #     - PYTHONUNBUFFERED=1
  #   depends_on: # Бэкенд зависит от баз данных и сервиса эмбеддингов
  #     postgres:
  #       condition: service_healthy # Ждем, пока PostgreSQL будет готов
  #     qdrant:
  #       condition: service_started # Ждем запуска Qdrant
  #     infinity:
  #       condition: service_started # Ждем запуска Infinity
  #   networks:
  #     - ai_tutor_net
  #   restart: unless-stopped

  # open-webui: # Сервис Open WebUI
  #   image: ghcr.io/open-webui/open-webui:main # Официальный образ
  #   container_name: open_webui
  #   ports:
  #     - "${OPEN_WEBUI_PORT:-3000}:8080" # Пробрасываем порт 8080 контейнера на порт хоста (по умолчанию 3000)
  #   environment:
  #     # Указываем URL нашего бэкенда (FastAPI), который предоставляет Ollama-совместимый API
  #     # Используем имя сервиса 'ai_tutor_backend' и порт 8000 внутри Docker сети
  #     - OLLAMA_BASE_URL=http://ai_tutor_backend:8000
  #     # Можно добавить другие переменные Open WebUI при необходимости
  #     # - WEBUI_SECRET_KEY= # Рекомендуется установить секретный ключ
  #   volumes:
  #     - open_webui_data:/app/backend/data # Сохранение данных Open WebUI
  #   depends_on: # Open WebUI зависит от нашего бэкенда
  #     ai_tutor_backend:
  #       condition: service_started # Ждем запуска бэкенда
  #   networks:
  #     - ai_tutor_net
    # restart: unless-stopped

# volumes:
#   postgres_data:
#     driver: local
#   open_webui_data: # Том для данных Open WebUI
#     driver: local
#   # .qdrant_storage и .infinity_cache используются как bind mounts

networks:
  ai_tutor_net:
    driver: bridge

