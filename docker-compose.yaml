services:
  sglang:
    image: lmsysorg/sglang:v0.4.6.post4-cu124
    container_name: sglang
    volumes:
      - /home/user/.cache/huggingface:/root/.cache/huggingface
      - /home/user/.cache/torch_compile:/root/.cache/torch_compile
    restart: unless-stopped
    network_mode: host
    shm_size: 16g
    environment:
      HF_TOKEN: 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
      TORCHINDUCTOR_CACHE_DIR: /root/.cache/torch_compile
    entrypoint: python3 -m sglang.launch_server
    command: >
      --model-path deepseek-ai/DeepSeek-R1-0528-Qwen3-8B --attention-backend
      flashinfer --host 0.0.0.0 --port 30000 --context-length 15000 --log-level
      info --show-time-cost --enable-metrics --enable-torch-compile
      --trust-remote-code --tool-call-parser qwen25 --reasoning-parser qwen3
      --mem-fraction-static 0.85
    ulimits:
      memlock: -1
      stack: 67108864
    ipc: host
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:30000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]

  postgres:
    image: pgvector/pgvector:pg14
    container_name: medical_bot_postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-admin}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-password}
      POSTGRES_DB: ${POSTGRES_DB:-medical_bot_db}
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - ./postgres_data_on_host:/var/lib/postgresql/data
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "pg_isready -U $${POSTGRES_USER:-admin} -d
          $${POSTGRES_DB:-medical_bot_db}",
        ]
      interval: 10s
      timeout: 5s
      retries: 5

  infinity:
    image: michaelf34/infinity:0.0.76
    container_name: infinity_rag
    restart: unless-stopped
    ports:
      - "7997:7997"
    volumes:
      - /home/user/Projects/data-extracting/.infinity_cache
    command:
      [
        "v2",
        "--model-id",
        "sergeyzh/BERTA",
        "--device",
        "cuda",
        "--batch-size",
        "20",
      ]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  qdrant:
    image: qdrant/qdrant:v1.14.0
    container_name: ai_tutor_qdrant
    ports:
      - "${QDRANT_HTTP_PORT:-6333}:6333"
      - "${QDRANT_GRPC_PORT:-6334}:6334"
    volumes:
      - ./.qdrant_storage:/qdrant/storage
    restart: unless-stopped

  telegram_bot:
    build:
      context: .
      dockerfile: DockerFile
    container_name: medical_telegram_bot
    restart: unless-stopped
    network_mode: "host"
    volumes:
      - ./logs:/app/logs
      - ./lightrag_data_docker:/app/app/lightrag_storage
