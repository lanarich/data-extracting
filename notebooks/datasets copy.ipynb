{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c50de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Optional, List, Dict, Any\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "from deepeval.models import DeepEvalBaseEmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0404087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfinityEmbeddingModel(DeepEvalBaseEmbeddingModel):\n",
    "    def __init__(self, model_name: str, base_url: str, api_key: Optional[str] = None):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key if api_key is not None else os.getenv(\"OPENAI_API_KEY\", \"123\")\n",
    "        self._sync_client: Optional[OpenAI] = None\n",
    "        self._async_client: Optional[AsyncOpenAI] = None\n",
    "\n",
    "    def load_model(self) -> OpenAI:\n",
    "        if self._sync_client is None:\n",
    "            self._sync_client = OpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "        return self._sync_client\n",
    "\n",
    "    def load_async_model(self) -> AsyncOpenAI:\n",
    "        if self._async_client is None:\n",
    "            self._async_client = AsyncOpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "        return self._async_client\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        client = self.load_model()\n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                model=self.model_name,\n",
    "                input=text\n",
    "            )\n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Error during synchronous Infinity embedding for text '{text[:50]}...': {e}\")\n",
    "            return []\n",
    "\n",
    "    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        client = self.load_model()\n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                model=self.model_name,\n",
    "                input=texts\n",
    "            )\n",
    "            return [data.embedding for data in response.data]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during synchronous Infinity batch embedding: {e}\")\n",
    "            return [[] for _ in texts] \n",
    "\n",
    "    async def a_embed_text(self, text: str) -> List[float]:\n",
    "        client = self.load_async_model()\n",
    "        try:\n",
    "            response = await client.embeddings.create(\n",
    "                model=self.model_name,\n",
    "                input=text\n",
    "            )\n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Error during asynchronous Infinity embedding for text '{text[:50]}...': {e}\")\n",
    "            return []\n",
    "\n",
    "    async def a_embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        client = self.load_async_model()\n",
    "        try:\n",
    "            response = await client.embeddings.create(\n",
    "                model=self.model_name,\n",
    "                input=texts\n",
    "            )\n",
    "            return [data.embedding for data in response.data]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during asynchronous Infinity batch embedding: {e}\")\n",
    "            return [[] for _ in texts]\n",
    "\n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns the name of the embedding model.\n",
    "        \"\"\"\n",
    "        return self.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c23cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import Optional, Union, Dict, List, Any\n",
    "from pydantic import BaseModel\n",
    "import instructor\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "\n",
    "class LLMModel(DeepEvalBaseLLM):\n",
    "    COGITO_THINKING_INSTRUCTION = \"Enable deep thinking subroutine.\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model_name: str,\n",
    "                 base_url: str,\n",
    "                 api_key: Optional[str] = None,\n",
    "                 attempt_thinking_mode: bool = False,\n",
    "                 cleaning_method: Optional[str] = None,\n",
    "                 max_tokens: int = 2000\n",
    "                ):\n",
    "        self.model_name_original = model_name\n",
    "        self.model_name_lower = model_name.lower()\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key if api_key is not None else os.getenv(\"OPENAI_API_KEY\", \"EMPTY\")\n",
    "        # Используйте instructor.patch сразу при создании клиентов\n",
    "        self._sync_client: Optional[OpenAI] = None\n",
    "        self._async_client: Optional[AsyncOpenAI] = None\n",
    "        self.attempt_thinking_mode = attempt_thinking_mode\n",
    "        self.cleaning_method = cleaning_method\n",
    "        self.max_tokens_to_generate = max_tokens\n",
    "        super().__init__(model_name=model_name)\n",
    "\n",
    "    def load_model(self) -> OpenAI:\n",
    "        if self._sync_client is None:\n",
    "            # Патчим клиент при создании\n",
    "            self._sync_client = instructor.patch(OpenAI(base_url=self.base_url, api_key=self.api_key))\n",
    "        return self._sync_client\n",
    "\n",
    "    def load_async_model(self) -> AsyncOpenAI:\n",
    "        if self._async_client is None:\n",
    "            # Патчим асинхронный клиент при создании\n",
    "            self._async_client = instructor.patch(AsyncOpenAI(base_url=self.base_url, api_key=self.api_key))\n",
    "        return self._async_client\n",
    "\n",
    "    def _clean_response(self, raw_response: str) -> str:\n",
    "        if not raw_response:\n",
    "            return \"\"\n",
    "\n",
    "        cleaned = raw_response.strip()\n",
    "\n",
    "        if self.cleaning_method == \"rsplit\":\n",
    "            closing_tag = '</think>'\n",
    "            if closing_tag in cleaned:\n",
    "                parts = cleaned.rsplit(closing_tag, 1)\n",
    "                think_block_start = parts[0].rfind('<think>')\n",
    "                if think_block_start != -1:\n",
    "                    cleaned = parts[0][:think_block_start].rstrip() + parts[1].lstrip()\n",
    "                else:\n",
    "                     cleaned = parts[1].lstrip()\n",
    "            return cleaned.strip()\n",
    "\n",
    "        elif self.cleaning_method == \"regex\":\n",
    "            pattern = r'<think>.*?</think>\\s*'\n",
    "            cleaned = re.sub(pattern, '', cleaned, flags=re.DOTALL).strip()\n",
    "            return cleaned\n",
    "        else:\n",
    "            return cleaned\n",
    "\n",
    "    def _prepare_api_call_args(self, prompt: str) -> Dict[str, Any]:\n",
    "        messages: List[Dict[str, str]] = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        api_extra_params: Dict[str, Any] = {}\n",
    "\n",
    "        if self.attempt_thinking_mode:\n",
    "            if \"cogito\" in self.model_name_lower:\n",
    "                messages.insert(0, {\"role\": \"system\", \"content\": self.COGITO_THINKING_INSTRUCTION})\n",
    "\n",
    "        return {\"messages\": messages, \"api_extra_params\": api_extra_params}\n",
    "\n",
    "    # --- ИЗМЕНЕННЫЙ МЕТОД generate ---\n",
    "    def generate(self, prompt: str, schema: Optional[BaseModel] = None) -> Union[str, BaseModel]:  \n",
    "        client = self.load_model()  \n",
    "        call_args = self._prepare_api_call_args(prompt)  \n",
    "        messages = call_args[\"messages\"]  \n",
    "        api_extra_params = call_args[\"api_extra_params\"]  \n",
    "    \n",
    "        try:  \n",
    "            # Если передана схема, используем response_model  \n",
    "            if schema is not None:  \n",
    "                response = client.chat.completions.create(  \n",
    "                    model=self.model_name_original,  \n",
    "                    messages=messages,  \n",
    "                    max_tokens=self.max_tokens_to_generate,  \n",
    "                    response_model=schema,  # Важно: передаем схему как response_model  \n",
    "                    **api_extra_params  \n",
    "                )  \n",
    "                return response  # Instructor возвращает уже объект схемы  \n",
    "            else:  \n",
    "                # Обычный вызов для текстового ответа  \n",
    "                response = client.chat.completions.create(  \n",
    "                    model=self.model_name_original,  \n",
    "                    messages=messages,  \n",
    "                    max_tokens=self.max_tokens_to_generate,  \n",
    "                    **api_extra_params  \n",
    "                )  \n",
    "                raw_response_content = response.choices[0].message.content  \n",
    "                cleaned_response = self._clean_response(raw_response_content)  \n",
    "                return cleaned_response  \n",
    "        except Exception as e:  \n",
    "            print(f\"Error during synchronous generation for model '{self.model_name_original}', prompt '{prompt[:50]}...': {e}\")  \n",
    "            if schema is not None:  \n",
    "                # Создаем заглушку с нулевыми значениями  \n",
    "                default_values = {}  \n",
    "                for field_name, field in schema.__annotations__.items():  \n",
    "                    if field_name in ['clarity', 'depth', 'structure', 'relevance']:  \n",
    "                        default_values[field_name] = 0.0  \n",
    "                    else:  \n",
    "                        default_values[field_name] = \"\"  \n",
    "                return schema(**default_values)  \n",
    "            return \"\"\n",
    "\n",
    "    # --- ИЗМЕНЕННЫЙ МЕТОД a_generate ---\n",
    "    async def a_generate(self, prompt: str, schema: Optional[BaseModel] = None) -> Union[str, BaseModel]:  \n",
    "        client = self.load_async_model()  \n",
    "        call_args = self._prepare_api_call_args(prompt)  \n",
    "        messages = call_args[\"messages\"]  \n",
    "        api_extra_params = call_args[\"api_extra_params\"]  \n",
    "    \n",
    "        try:  \n",
    "            # Если передана схема, используем response_model  \n",
    "            if schema is not None:  \n",
    "                response = await client.chat.completions.create(  \n",
    "                    model=self.model_name_original,  \n",
    "                    messages=messages,  \n",
    "                    max_tokens=self.max_tokens_to_generate,  \n",
    "                    response_model=schema,  # Важно: передаем схему как response_model  \n",
    "                    **api_extra_params  \n",
    "                )  \n",
    "                return response  # Instructor возвращает уже объект схемы  \n",
    "            else:  \n",
    "                # Обычный вызов для текстового ответа  \n",
    "                response = await client.chat.completions.create(  \n",
    "                    model=self.model_name_original,  \n",
    "                    messages=messages,  \n",
    "                    max_tokens=self.max_tokens_to_generate,  \n",
    "                    **api_extra_params  \n",
    "                )  \n",
    "                raw_response_content = response.choices[0].message.content  \n",
    "                cleaned_response = self._clean_response(raw_response_content)  \n",
    "                return cleaned_response  \n",
    "        except Exception as e:  \n",
    "            print(f\"Error during asynchronous generation for model '{self.model_name_original}', prompt '{prompt[:50]}...': {e}\")  \n",
    "            if schema is not None:  \n",
    "                # Создаем заглушку с нулевыми значениями  \n",
    "                default_values = {}  \n",
    "                for field_name, field in schema.__annotations__.items():  \n",
    "                    if field_name in ['clarity', 'depth', 'structure', 'relevance']:  \n",
    "                        default_values[field_name] = 0.0  \n",
    "                    else:  \n",
    "                        default_values[field_name] = \"\"  \n",
    "                return schema(**default_values)  \n",
    "            return \"\"\n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        return self.model_name_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  \n",
    "import os  \n",
    "import json  \n",
    "from typing import Optional, Union, Tuple, Dict  \n",
    "from deepeval.models import DeepEvalBaseLLM  \n",
    "from openai import OpenAI, AsyncOpenAI  \n",
    "from pydantic import BaseModel  \n",
    "from lmformatenforcer import JsonSchemaParser  \n",
    "  \n",
    "class SGlangModel(DeepEvalBaseLLM):  \n",
    "    def __init__(self,   \n",
    "                 model_name: str,   \n",
    "                 base_url: str,   \n",
    "                 api_key: Optional[str] = \"NET\",  \n",
    "                 enable_thinking: bool = False):  \n",
    "        \"\"\"  \n",
    "        Инициализирует модель SGlang.  \n",
    "  \n",
    "        Args:  \n",
    "            model_name (str): Имя модели.  \n",
    "            base_url (str): Базовый URL для API.  \n",
    "            api_key (Optional[str]): API ключ. По умолчанию \"NET\".  \n",
    "            enable_thinking (bool): Флаг для управления поведением моделей Qwen3.  \n",
    "                                     Если True, к промпту для Qwen3 будет добавлен \"/think\".  \n",
    "                                     Если False, к промпту для Qwen3 будет добавлен \"/no_think\".  \n",
    "                                     По умолчанию False.  \n",
    "        \"\"\"  \n",
    "        self.model_name = model_name  \n",
    "        self.base_url = base_url  \n",
    "        self.api_key = api_key if api_key is not None else os.getenv(\"OPENAI_API_KEY\")  \n",
    "        self.enable_thinking = enable_thinking   \n",
    "        self._sync_client: Optional[OpenAI] = None  \n",
    "        self._async_client: Optional[AsyncOpenAI] = None  \n",
    "  \n",
    "    def load_model(self) -> OpenAI:  \n",
    "        if self._sync_client is None:  \n",
    "            self._sync_client = OpenAI(base_url=self.base_url, api_key=self.api_key)  \n",
    "        return self._sync_client  \n",
    "  \n",
    "    def load_async_model(self) -> AsyncOpenAI:  \n",
    "        if self._async_client is None:  \n",
    "            self._async_client = AsyncOpenAI(base_url=self.base_url, api_key=self.api_key)  \n",
    "        return self._async_client  \n",
    "  \n",
    "    def _clean_qwen3_output(self, text_response: str) -> str:  \n",
    "        \"\"\"  \n",
    "        Удаляет начальный блок <think>...</think> из ответов модели Qwen3.  \n",
    "        \"\"\"  \n",
    "        pattern = r'^\\s*<think>.*?</think>\\s*'  \n",
    "        cleaned_response = re.sub(pattern, '', text_response, count=1, flags=re.DOTALL)  \n",
    "        return cleaned_response  \n",
    "  \n",
    "    def _trim_and_load_json(self, input_string: str) -> Dict:  \n",
    "        \"\"\"  \n",
    "        Обрабатывает строку JSON, удаляя лишние символы и преобразуя в словарь.  \n",
    "        \"\"\"  \n",
    "        start = input_string.find(\"{\")  \n",
    "        end = input_string.rfind(\"}\") + 1  \n",
    "        if end == 0 and start != -1:  \n",
    "            input_string = input_string + \"}\"  \n",
    "            end = len(input_string)  \n",
    "        jsonStr = input_string[start:end] if start != -1 and end != 0 else \"\"  \n",
    "        jsonStr = re.sub(r\",\\s*([\\]}])\", r\"\\1\", jsonStr)  \n",
    "        try:  \n",
    "            return json.loads(jsonStr)  \n",
    "        except json.JSONDecodeError:  \n",
    "            error_str = \"Модель вывела некорректный JSON. Пожалуйста, используйте более надежную модель.\"  \n",
    "            raise ValueError(error_str)  \n",
    "        except Exception as e:  \n",
    "            raise Exception(f\"Произошла непредвиденная ошибка: {str(e)}\")  \n",
    "  \n",
    "    def generate(self, prompt: str, schema: Optional[BaseModel] = None) -> Tuple[Union[str, BaseModel], float]:  \n",
    "        \"\"\"  \n",
    "        Генерирует ответ от модели.   \n",
    "        Для модели 'Qwen3' (без схемы):  \n",
    "        - К промпту добавляется \"/think\" или \"/no_think\" в зависимости от self.enable_thinking.  \n",
    "        - Блок <think> всегда удаляется из финального ответа.  \n",
    "          \n",
    "        Returns:  \n",
    "            Tuple[Union[str, BaseModel], float]: Кортеж (результат, стоимость)  \n",
    "        \"\"\"  \n",
    "        client = self.load_model()  \n",
    "          \n",
    "        processed_prompt = prompt  \n",
    "        # Проверка на Qwen3 (нечувствительная к регистру) и отсутствие схемы  \n",
    "        is_qwen3_text_mode = \"qwen3\" in self.model_name.lower() and schema is None  \n",
    "  \n",
    "        if is_qwen3_text_mode:  \n",
    "            # Удаляем существующие теги /think или /no_think из конца промпта  \n",
    "            processed_prompt = re.sub(r'\\s*/think\\s*$', '', processed_prompt, flags=re.IGNORECASE).strip()  \n",
    "            processed_prompt = re.sub(r'\\s*/no_think\\s*$', '', processed_prompt, flags=re.IGNORECASE).strip()  \n",
    "              \n",
    "            if self.enable_thinking:  \n",
    "                processed_prompt += \" /think\" # Инструктируем Qwen3 выполнить процесс размышления  \n",
    "            else:  \n",
    "                processed_prompt += \" /no_think\" # Инструктируем Qwen3 пропустить/минимизировать размышления  \n",
    "          \n",
    "        try:  \n",
    "            if schema is None:  \n",
    "                response = client.chat.completions.create(  \n",
    "                    model=self.model_name,  \n",
    "                    messages=[{\"role\": \"user\", \"content\": processed_prompt}],   \n",
    "                )  \n",
    "                raw_content = response.choices[0].message.content  \n",
    "                  \n",
    "                # Рассчитываем стоимость, если доступно  \n",
    "                cost = 0  \n",
    "                if hasattr(response, 'usage') and hasattr(response.usage, 'total_tokens'):  \n",
    "                    cost = 0  # Замените на реальный расчет стоимости  \n",
    "                  \n",
    "                if is_qwen3_text_mode:  \n",
    "                    # Для Qwen3 в текстовом режиме всегда очищаем вывод от блока <think>  \n",
    "                    return self._clean_qwen3_output(raw_content), cost  \n",
    "                else:  \n",
    "                    # Для других моделей возвращаем \"сырой\" контент  \n",
    "                    return raw_content, cost  \n",
    "            else:  \n",
    "                # Используем lm-format-enforcer через vLLM API  \n",
    "                if \"vllm\" in self.base_url.lower():  \n",
    "                    # Если используется vLLM, используем встроенную поддержку lm-format-enforcer  \n",
    "                    response = client.chat.completions.create(  \n",
    "                        model=self.model_name,  \n",
    "                        messages=[{\"role\": \"user\", \"content\": processed_prompt}],  \n",
    "                        extra_body={  \n",
    "                            \"guided_json\": schema.model_json_schema(),  \n",
    "                            \"guided_decoding_backend\": \"lm-format-enforcer\"  \n",
    "                        }  \n",
    "                    )  \n",
    "                    raw_content = response.choices[0].message.content  \n",
    "                      \n",
    "                    # Рассчитываем стоимость, если доступно  \n",
    "                    cost = 0  \n",
    "                    if hasattr(response, 'usage') and hasattr(response.usage, 'total_tokens'):  \n",
    "                        cost = 0  # Замените на реальный расчет стоимости  \n",
    "                      \n",
    "                    # Преобразуем JSON-строку в объект схемы  \n",
    "                    json_data = self._trim_and_load_json(raw_content)  \n",
    "                    return schema.model_validate(json_data), cost  \n",
    "                else:  \n",
    "                    # Для других API используем обычный запрос с последующей валидацией  \n",
    "                    # Добавляем инструкцию для генерации JSON в соответствии со схемой  \n",
    "                    schema_json = json.dumps(schema.model_json_schema(), indent=2)  \n",
    "                    json_prompt = f\"{processed_prompt}\\n\\nОтвет должен быть в формате JSON, соответствующем следующей схеме:\\n{schema_json}\\n\\nВажно: Ответ должен содержать только валидный JSON без дополнительного текста.\"  \n",
    "                      \n",
    "                    response = client.chat.completions.create(  \n",
    "                        model=self.model_name,  \n",
    "                        messages=[{\"role\": \"user\", \"content\": json_prompt}],  \n",
    "                    )  \n",
    "                    raw_content = response.choices[0].message.content  \n",
    "                      \n",
    "                    # Рассчитываем стоимость, если доступно  \n",
    "                    cost = 0  \n",
    "                    if hasattr(response, 'usage') and hasattr(response.usage, 'total_tokens'):  \n",
    "                        cost = 0  # Замените на реальный расчет стоимости  \n",
    "                      \n",
    "                    # Обрабатываем и валидируем JSON  \n",
    "                    json_data = self._trim_and_load_json(raw_content)  \n",
    "                    return schema.model_validate(json_data), cost  \n",
    "        except Exception as e:  \n",
    "            print(f\"Ошибка при синхронной генерации для промпта '{prompt[:50]}...': {e}\")  \n",
    "            raise e  \n",
    "  \n",
    "    async def a_generate(self, prompt: str, schema: Optional[BaseModel] = None) -> Tuple[Union[str, BaseModel], float]:  \n",
    "        \"\"\"  \n",
    "        Асинхронно генерирует ответ от модели.  \n",
    "        Для модели 'Qwen3' (без схемы):  \n",
    "        - К промпту добавляется \"/think\" или \"/no_think\" в зависимости от self.enable_thinking.  \n",
    "        - Блок <think> всегда удаляется из финального ответа.  \n",
    "          \n",
    "        Returns:  \n",
    "            Tuple[Union[str, BaseModel], float]: Кортеж (результат, стоимость)  \n",
    "        \"\"\"  \n",
    "        client = self.load_async_model()  \n",
    "  \n",
    "        processed_prompt = prompt  \n",
    "        # Проверка на Qwen3 (нечувствительная к регистру) и отсутствие схемы  \n",
    "        is_qwen3_text_mode = \"qwen3\" in self.model_name.lower() and schema is None  \n",
    "  \n",
    "        if is_qwen3_text_mode:  \n",
    "            # Удаляем существующие теги /think или /no_think из конца промпта  \n",
    "            processed_prompt = re.sub(r'\\s*/think\\s*$', '', processed_prompt, flags=re.IGNORECASE).strip()  \n",
    "            processed_prompt = re.sub(r'\\s*/no_think\\s*$', '', processed_prompt, flags=re.IGNORECASE).strip()  \n",
    "  \n",
    "            if self.enable_thinking:  \n",
    "                processed_prompt += \" /think\" # Инструктируем Qwen3 выполнить процесс размышления  \n",
    "            else:  \n",
    "                processed_prompt += \" /no_think\" # Инструктируем Qwen3 пропустить/минимизировать размышления  \n",
    "  \n",
    "        try:  \n",
    "            if schema is None:  \n",
    "                response = await client.chat.completions.create(  \n",
    "                    model=self.model_name,  \n",
    "                    messages=[{\"role\": \"user\", \"content\": processed_prompt}],   \n",
    "                )  \n",
    "                raw_content = response.choices[0].message.content  \n",
    "                  \n",
    "                # Рассчитываем стоимость, если доступно  \n",
    "                cost = 0  \n",
    "                if hasattr(response, 'usage') and hasattr(response.usage, 'total_tokens'):  \n",
    "                    cost = 0  # Замените на реальный расчет стоимости  \n",
    "  \n",
    "                if is_qwen3_text_mode:  \n",
    "                    # Для Qwen3 в текстовом режиме всегда очищаем вывод от блока <think>  \n",
    "                    return self._clean_qwen3_output(raw_content), cost  \n",
    "                else:  \n",
    "                    # Для других моделей возвращаем \"сырой\" контент  \n",
    "                    return raw_content, cost  \n",
    "            else:  \n",
    "                # Используем lm-format-enforcer через vLLM API  \n",
    "                if \"vllm\" in self.base_url.lower():  \n",
    "                    # Если используется vLLM, используем встроенную поддержку lm-format-enforcer  \n",
    "                    response = await client.chat.completions.create(  \n",
    "                        model=self.model_name,  \n",
    "                        messages=[{\"role\": \"user\", \"content\": processed_prompt}],  \n",
    "                        extra_body={  \n",
    "                            \"guided_json\": schema.model_json_schema(),  \n",
    "                            \"guided_decoding_backend\": \"lm-format-enforcer\"  \n",
    "                        }  \n",
    "                    )  \n",
    "                    raw_content = response.choices[0].message.content  \n",
    "                      \n",
    "                    # Рассчитываем стоимость, если доступно  \n",
    "                    cost = 0  \n",
    "                    if hasattr(response, 'usage') and hasattr(response.usage, 'total_tokens'):  \n",
    "                        cost = 0  # Замените на реальный расчет стоимости  \n",
    "                      \n",
    "                    # Преобразуем JSON-строку в объект схемы  \n",
    "                    json_data = self._trim_and_load_json(raw_content)  \n",
    "                    return schema.model_validate(json_data), cost  \n",
    "                else:  \n",
    "                    # Для других API используем обычный запрос с последующей валидацией  \n",
    "                    # Добавляем инструкцию для генерации JSON в соответствии со схемой  \n",
    "                    schema_json = json.dumps(schema.model_json_schema(), indent=2)  \n",
    "                    json_prompt = f\"{processed_prompt}\\n\\nОтвет должен быть в формате JSON, соответствующем следующей схеме:\\n{schema_json}\\n\\nВажно: Ответ должен содержать только валидный JSON без дополнительного текста.\"  \n",
    "                      \n",
    "                    response = await client.chat.completions.create(  \n",
    "                        model=self.model_name,  \n",
    "                        messages=[{\"role\": \"user\", \"content\": json_prompt}],  \n",
    "                    )  \n",
    "                    raw_content = response.choices[0].message.content  \n",
    "                      \n",
    "                    # Рассчитываем стоимость, если доступно  \n",
    "                    cost = 0  \n",
    "                    if hasattr(response, 'usage') and hasattr(response.usage, 'total_tokens'):  \n",
    "                        cost = 0  # Замените на реальный расчет стоимости  \n",
    "                      \n",
    "                    # Обрабатываем и валидируем JSON  \n",
    "                    json_data = self._trim_and_load_json(raw_content)  \n",
    "                    return schema.model_validate(json_data), cost  \n",
    "        except Exception as e:  \n",
    "            print(f\"Ошибка при асинхронной генерации для промпта '{prompt[:50]}...': {e}\")  \n",
    "            raise e  \n",
    "  \n",
    "    def get_model_name(self) -> str:  \n",
    "        return self.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bf41d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Optional, List, Dict, Any\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "\n",
    "class SGlangModel(DeepEvalBaseLLM):\n",
    "    \"\"\"\n",
    "    Generic DeepEval LLM wrapper for models served via an\n",
    "    OpenAI-compatible API (e.g., SGLang, vLLM).\n",
    "    Can attempt to enable model-specific thinking/reasoning modes\n",
    "    based on model_name and removes <think>...</think> tags from responses if they appear.\n",
    "    \"\"\"\n",
    "    # Специфическая инструкция для Cogito\n",
    "    COGITO_THINKING_INSTRUCTION = \"Enable deep thinking subroutine.\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model_name: str, # Имя модели, используется для логики\n",
    "                 base_url: str,   # URL эндпоинта OpenAI-совместимого API\n",
    "                 api_key: Optional[str] = None, # API ключ (часто \"EMPTY\" или не нужен для локальных)\n",
    "                 attempt_thinking_mode: bool = False, # Пытаться ли включить режим рассуждений?\n",
    "                 cleaning_method: str = \"rsplit\", # Метод очистки тегов: 'rsplit' или 'regex'\n",
    "                 max_tokens: int = 8192 # Макс. токенов для генерации\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initializes the SGlangModel wrapper.\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the model being served (e.g., \"Qwen/Qwen3-30B-A3B\", \"deepcogito/cogito-v1...\").\n",
    "            base_url: The base URL of the OpenAI-compatible API endpoint.\n",
    "            api_key: Optional API key for the endpoint. Defaults to env variable or \"EMPTY\".\n",
    "            attempt_thinking_mode: If True, tries to enable thinking mode based on model_name. Defaults to False.\n",
    "            cleaning_method: Method to use for removing <think> tags ('rsplit' or 'regex'). Defaults to 'rsplit'.\n",
    "            max_tokens: Maximum number of new tokens to generate. Defaults to 8192.\n",
    "        \"\"\"\n",
    "        self.model_name_original = model_name # Сохраняем оригинальное имя\n",
    "        self.model_name_lower = model_name.lower() # Сохраняем в нижнем регистре для сравнения\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key if api_key is not None else os.getenv(\"OPENAI_API_KEY\", \"EMPTY\")\n",
    "        self._sync_client: Optional[OpenAI] = None\n",
    "        self._async_client: Optional[AsyncOpenAI] = None\n",
    "        self.attempt_thinking_mode = attempt_thinking_mode\n",
    "        self.cleaning_method = cleaning_method\n",
    "        self.max_tokens_to_generate = max_tokens\n",
    "\n",
    "    def load_model(self) -> OpenAI:\n",
    "        \"\"\"Loads or returns the synchronous OpenAI client.\"\"\"\n",
    "        if self._sync_client is None:\n",
    "            self._sync_client = OpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "        return self._sync_client\n",
    "\n",
    "    def load_async_model(self) -> AsyncOpenAI:\n",
    "        \"\"\"Loads or returns the asynchronous OpenAI client.\"\"\"\n",
    "        if self._async_client is None:\n",
    "            self._async_client = AsyncOpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "        return self._async_client\n",
    "\n",
    "    def _clean_response(self, raw_response: str) -> str:\n",
    "        \"\"\"\n",
    "        Helper function to remove <think>...</think> blocks from the raw response string.\n",
    "        Uses the method specified in self.cleaning_method.\n",
    "        \"\"\"\n",
    "        if not raw_response:\n",
    "            return \"\"\n",
    "\n",
    "        if self.cleaning_method == \"rsplit\":\n",
    "            closing_tag = '</think>'\n",
    "            if closing_tag in raw_response:\n",
    "                # Разделяем по последнему тегу и берем правую часть\n",
    "                main_answer = raw_response.rsplit(closing_tag, 1)[-1].strip()\n",
    "            else:\n",
    "                # Если тега нет, просто убираем пробелы по краям\n",
    "                main_answer = raw_response.strip()\n",
    "            return main_answer\n",
    "\n",
    "        elif self.cleaning_method == \"regex\":\n",
    "            # Шаблон для удаления <think>...</think> и пробелов после\n",
    "            pattern = r'<think>.*?</think>\\s*'\n",
    "            # Заменяем найденное на пустую строку, DOTALL для переносов строк\n",
    "            main_answer = re.sub(pattern, '', raw_response, flags=re.DOTALL).strip()\n",
    "            return main_answer\n",
    "        else:\n",
    "            # Если метод не 'rsplit' и не 'regex', возвращаем как есть, убрав пробелы\n",
    "            print(f\"Warning: Unknown cleaning_method '{self.cleaning_method}'. Returning raw response.\")\n",
    "            return raw_response.strip()\n",
    "\n",
    "    def _prepare_api_call_args(self, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Prepares the messages list and a dictionary of extra API parameters\n",
    "        based on the model name and the attempt_thinking_mode flag.\n",
    "        \"\"\"\n",
    "        # Базовый список сообщений - только промпт пользователя\n",
    "        messages: List[Dict[str, str]] = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        # Словарь для дополнительных параметров API (например, enable_thinking)\n",
    "        api_extra_params: Dict[str, Any] = {}\n",
    "\n",
    "        if self.attempt_thinking_mode:\n",
    "            # --- Логика для конкретных моделей ---\n",
    "            if \"cogito\" in self.model_name_lower:\n",
    "                # Добавляем системный промпт для Cogito В НАЧАЛО списка\n",
    "                messages.insert(0, {\"role\": \"system\", \"content\": self.COGITO_THINKING_INSTRUCTION})\n",
    "                print(f\"Info: Enabling Cogito thinking mode via system prompt for model '{self.model_name_original}'.\")\n",
    "\n",
    "            elif \"qwen3\" in self.model_name_lower:\n",
    "                # Добавляем параметр API для Qwen3\n",
    "                # ВАЖНО: Имя параметра 'enable_thinking' является ПРЕДПОЛОЖЕНИЕМ.\n",
    "                # Проверьте документацию вашего сервера SGLang/vLLM!\n",
    "                # Если возникнет ошибка, возможно, потребуется использовать 'extra_body'.\n",
    "                api_extra_params[\"enable_thinking\"] = True\n",
    "                print(f\"Info: Attempting to enable Qwen3 thinking mode via API parameter for model '{self.model_name_original}'.\")\n",
    "\n",
    "            else:\n",
    "                # Модель не распознана для специальной обработки режима рассуждений\n",
    "                print(f\"Warning: 'attempt_thinking_mode' is True, but no specific handling defined for model '{self.model_name_original}'. Making standard call.\")\n",
    "            # --- Конец логики для моделей ---\n",
    "\n",
    "        # Возвращаем словарь с подготовленными сообщениями и доп. параметрами\n",
    "        return {\"messages\": messages, \"api_extra_params\": api_extra_params}\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates a response synchronously.\n",
    "        If attempt_thinking_mode is True, it modifies the request based on the model.\n",
    "        It always cleans the response to remove <think> tags.\n",
    "        \"\"\"\n",
    "        client = self.load_model()\n",
    "        # Подготавливаем аргументы для вызова API\n",
    "        call_args = self._prepare_api_call_args(prompt)\n",
    "        messages = call_args[\"messages\"]\n",
    "        api_extra_params = call_args[\"api_extra_params\"]\n",
    "\n",
    "        try:\n",
    "            # Выполняем вызов API, передавая основные и дополнительные параметры\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model_name_original, # Используем оригинальное имя модели\n",
    "                messages=messages,\n",
    "                max_tokens=self.max_tokens_to_generate,\n",
    "                **api_extra_params # Распаковываем доп. параметры (может быть пустым)\n",
    "                # Примечание: Если 'enable_thinking' не работает как прямой параметр,\n",
    "                # попробуйте передать его так (закомментировав строку выше с **api_extra_params):\n",
    "                # extra_body=api_extra_params if api_extra_params else None\n",
    "            )\n",
    "            # Получаем сырой текстовый ответ\n",
    "            raw_response_content = response.choices[0].message.content\n",
    "            # Всегда очищаем ответ от тегов <think>\n",
    "            cleaned_response = self._clean_response(raw_response_content)\n",
    "            return cleaned_response\n",
    "        except Exception as e:\n",
    "            print(f\"Error during synchronous generation for model '{self.model_name_original}', prompt '{prompt[:50]}...': {e}\")\n",
    "            # Добавляем подсказки, если ошибка связана с параметром 'enable_thinking'\n",
    "            if api_extra_params.get(\"enable_thinking\") and \"unexpected keyword argument 'enable_thinking'\" in str(e).lower():\n",
    "                 print(\"Hint: The API might not support 'enable_thinking' as a direct parameter. Try modifying the wrapper to use 'extra_body'.\")\n",
    "            elif api_extra_params.get(\"enable_thinking\") and \"extra_body\" in str(e).lower():\n",
    "                 print(\"Hint: Check the exact API documentation for SGLang/vLLM OpenAI-compatible endpoint for enabling Qwen3 thinking mode.\")\n",
    "            return \"\" # Возвращаем пустую строку в случае ошибки\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates a response asynchronously.\n",
    "        If attempt_thinking_mode is True, it modifies the request based on the model.\n",
    "        It always cleans the response to remove <think> tags.\n",
    "        \"\"\"\n",
    "        client = self.load_async_model()\n",
    "        # Подготавливаем аргументы для вызова API\n",
    "        call_args = self._prepare_api_call_args(prompt)\n",
    "        messages = call_args[\"messages\"]\n",
    "        api_extra_params = call_args[\"api_extra_params\"]\n",
    "\n",
    "        try:\n",
    "            # Выполняем асинхронный вызов API\n",
    "            response = await client.chat.completions.create(\n",
    "                model=self.model_name_original, # Используем оригинальное имя модели\n",
    "                messages=messages,\n",
    "                max_tokens=self.max_tokens_to_generate,\n",
    "                **api_extra_params # Распаковываем доп. параметры\n",
    "                 # Примечание: Если 'enable_thinking' не работает как прямой параметр,\n",
    "                 # попробуйте передать его так (закомментировав строку выше с **api_extra_params):\n",
    "                 # extra_body=api_extra_params if api_extra_params else None\n",
    "            )\n",
    "            # Получаем сырой текстовый ответ\n",
    "            raw_response_content = response.choices[0].message.content\n",
    "            # Всегда очищаем ответ от тегов <think>\n",
    "            cleaned_response = self._clean_response(raw_response_content)\n",
    "            return cleaned_response\n",
    "        except Exception as e:\n",
    "            print(f\"Error during asynchronous generation for model '{self.model_name_original}', prompt '{prompt[:50]}...': {e}\")\n",
    "            # Добавляем аналогичные подсказки про enable_thinking / extra_body\n",
    "            if api_extra_params.get(\"enable_thinking\") and \"unexpected keyword argument 'enable_thinking'\" in str(e).lower():\n",
    "                 print(\"Hint: The API might not support 'enable_thinking' as a direct parameter. Try modifying the wrapper to use 'extra_body'.\")\n",
    "            elif api_extra_params.get(\"enable_thinking\") and \"extra_body\" in str(e).lower():\n",
    "                 print(\"Hint: Check the exact API documentation for SGLang/vLLM OpenAI-compatible endpoint for enabling Qwen3 thinking mode.\")\n",
    "            return \"\" # Возвращаем пустую строку в случае ошибки\n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns the original name of the model used for initialization.\n",
    "        \"\"\"\n",
    "        # Возвращаем оригинальное имя, сохраненное при инициализации\n",
    "        return self.model_name_original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df65fceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CogitoLLM= SGlangModel(model_name=\"deepcogito/cogito-v1-preview-llama-8B\", base_url=\"http://85.143.167.11:30000/v1\")\n",
    "\n",
    "# Qwen3_30_Reasoning = LLMModel(model_name=\"qwen3-30-lmstudio\", base_url=\"http://85.143.167.11:30000/v1\", attempt_thinking_mode=True, cleaning_method=\"rsplit\")\n",
    "# Qwen3_30 = LLMModel(model_name=\"qwen3-30-lmstudio\", base_url=\"http://85.143.167.11:30000/v1\", attempt_thinking_mode=False)\n",
    "\n",
    "# Qwen3_32_Reasoning = LLMModel(model_name=\"Qwen/Qwen3-32B-AWQ\", base_url=\"http://85.143.167.11:30000/v1\", attempt_thinking_mode=True, cleaning_method=\"rsplit\")\n",
    "# Qwen3_32 = LLMModel(model_name=\"Qwen/Qwen3-32B-AWQ\", base_url=\"http://85.143.167.11:30000/v1\", attempt_thinking_mode=False)\n",
    "\n",
    "# Qwen3_8_Reasoning = LLMModel(model_name=\"Qwen/Qwen3-8B-FP8\", base_url=\"http://85.143.167.11:30000/v1\", attempt_thinking_mode=True, cleaning_method=\"rsplit\")\n",
    "# Qwen3_8 = LLMModel(model_name=\"Qwen/Qwen3-8B-FP8\", base_url=\"http://85.143.167.11:30000/v1\", attempt_thinking_mode=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d76459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BertaEmbeddings = InfinityEmbeddingModel(model_name=\"sergeyzh/BERTA\", base_url=\"http://127.0.0.1:7997\")\n",
    "USER2Embeddings = InfinityEmbeddingModel(model_name=\"deepvk/USER2-base\", base_url=\"http://127.0.0.1:7997\")\n",
    "RuEnROBERTAEmbeddings = InfinityEmbeddingModel(model_name=\"ai-forever/ru-en-RoSBERTa\", base_url=\"http://127.0.0.1:7997\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79b80e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.synthesizer import Synthesizer, Evolution  \n",
    "from deepeval.synthesizer.config import StylingConfig, EvolutionConfig, ContextConstructionConfig, FiltrationConfig  \n",
    "  \n",
    "context_construction_config = ContextConstructionConfig(  \n",
    "    chunk_size=2048,                 \n",
    "    chunk_overlap=0,            \n",
    "    max_contexts_per_document=3,\n",
    "    context_quality_threshold=0.4,\n",
    "    max_retries=2,\n",
    "    critic_model = CogitoLLM,\n",
    "    embedder = BertaEmbeddings)  \n",
    "  \n",
    "styling_config = StylingConfig(  \n",
    "    input_format=\"\"\"Сложные академические вопросы на русском языке, основанные на содержании медицинских учебников.   \n",
    "    Вопросы должны быть сформулированы с использованием точной медицинской терминологии,   \n",
    "    требовать глубокого понимания материала и проверять способность применять теоретические знания   \n",
    "    к клиническим ситуациям.\"\"\",  \n",
    "      \n",
    "    expected_output_format=\"\"\"Структурированные, научно обоснованные ответы на русском языке,   \n",
    "    включающие ключевые концепции из учебника, точные определения, классификации и механизмы.   \n",
    "    Ответы должны быть организованы в логической последовательности с использованием подзаголовков, где это уместно.\"\"\",  \n",
    "      \n",
    "    task=\"\"\"Создание высококачественных экзаменационных вопросов на русском языке,   \n",
    "    которые точно отражают содержание учебника и проверяют глубину понимания материала.\"\"\",  \n",
    "      \n",
    "    scenario=\"\"\"Преподаватель медицинского вуза создает банк вопросов для экзаменов,   \n",
    "    тестирования и самопроверки студентов. Вопросы должны точно соответствовать   \n",
    "    содержанию учебников и быть пригодными для оценки компетенций студентов   \n",
    "    разных курсов медицинского образования.\"\"\",  \n",
    ")  \n",
    "  \n",
    "evolution_config = EvolutionConfig(  \n",
    "    num_evolutions=2, \n",
    "    evolutions={  \n",
    "        Evolution.CONCRETIZING: 0.25,     \n",
    "        Evolution.MULTICONTEXT: 0.25,   \n",
    "        Evolution.COMPARATIVE: 0.25,   \n",
    "        Evolution.CONSTRAINED: 0.25,\n",
    "    }  \n",
    ") \n",
    "\n",
    "filtration_config = FiltrationConfig(  \n",
    "    synthetic_input_quality_threshold=0.5,\n",
    "    critic_model= CogitoLLM,\n",
    "    max_quality_retries=2                \n",
    ")\n",
    "\n",
    "synthesizer = Synthesizer(\n",
    "    model=CogitoLLM,  \n",
    "    styling_config=styling_config,  \n",
    "    # evolution_config=evolution_config,\n",
    "    # filtration_config=filtration_config,  \n",
    "    async_mode=True,  \n",
    "    max_concurrent=8\n",
    ")  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f92b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "✨ 🚀 ✨ Loading Documents: 100%|██████████| 2/2 [00:02<00:00,  1.00s/it]\n",
      "✨ 📚 ✨ Chunking Documents: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]\n",
      "✨ 🧩 ✨ Generating Contexts:   0%|          | 0/12 [00:00<?, ?it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Evaluation LLM outputted an invalid JSON. Please use a better evaluation model.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/metrics/utils.py:274\u001b[39m, in \u001b[36mtrimAndLoadJson\u001b[39m\u001b[34m(input_string, metric)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjsonStr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m json.JSONDecodeError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/json/decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m end = _w(s, end).end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/json/decoder.py:355\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m goldens = \u001b[38;5;28;01mawait\u001b[39;00m synthesizer.a_generate_goldens_from_docs(  \n\u001b[32m      2\u001b[39m     document_paths=[\u001b[33m'\u001b[39m\u001b[33m/mnt/sdb1/PycharmProjects/CODUP/AI-tutor-other/docs/for_golds/Anatomia_cheloveka_1_tom_2-52-57.pdf\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      3\u001b[39m                     \u001b[33m'\u001b[39m\u001b[33m/mnt/sdb1/PycharmProjects/CODUP/AI-tutor-other/docs/for_golds/Kapandzhi_-_Pozvonochnik-276-284.pdf\u001b[39m\u001b[33m'\u001b[39m],  \n\u001b[32m      4\u001b[39m     include_expected_output=\u001b[38;5;28;01mTrue\u001b[39;00m,  \n\u001b[32m      5\u001b[39m     max_goldens_per_context=\u001b[32m3\u001b[39m,  \n\u001b[32m      6\u001b[39m     context_construction_config=context_construction_config  \n\u001b[32m      7\u001b[39m )  \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/synthesizer.py:220\u001b[39m, in \u001b[36mSynthesizer.a_generate_goldens_from_docs\u001b[39m\u001b[34m(self, document_paths, include_expected_output, max_goldens_per_context, context_construction_config, _reset_cost)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# Generate contexts from provided docs\u001b[39;00m\n\u001b[32m    208\u001b[39m context_generator = ContextGenerator(\n\u001b[32m    209\u001b[39m     document_paths=document_paths,\n\u001b[32m    210\u001b[39m     encoding=context_construction_config.encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    217\u001b[39m     max_retries=context_construction_config.max_retries,\n\u001b[32m    218\u001b[39m )\n\u001b[32m    219\u001b[39m contexts, source_files, context_scores = (\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m context_generator.a_generate_contexts(\n\u001b[32m    221\u001b[39m         max_contexts_per_source_file=context_construction_config.max_contexts_per_document,\n\u001b[32m    222\u001b[39m         min_contexts_per_source_file=context_construction_config.min_contexts_per_document,\n\u001b[32m    223\u001b[39m         max_context_size=context_construction_config.max_context_length,\n\u001b[32m    224\u001b[39m         min_context_size=context_construction_config.min_context_length,\n\u001b[32m    225\u001b[39m     )\n\u001b[32m    226\u001b[39m )\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.synthesis_cost:\n\u001b[32m    228\u001b[39m     \u001b[38;5;28mself\u001b[39m.synthesis_cost += context_generator.total_cost\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:241\u001b[39m, in \u001b[36mContextGenerator.a_generate_contexts\u001b[39m\u001b[34m(self, max_contexts_per_source_file, min_contexts_per_source_file, max_context_size, min_context_size)\u001b[39m\n\u001b[32m    230\u001b[39m     max_context_size = \u001b[38;5;28mmin\u001b[39m(max_context_size, collection.count())\n\u001b[32m    231\u001b[39m     tasks.append(\n\u001b[32m    232\u001b[39m         \u001b[38;5;28mself\u001b[39m._a_process_document_async(\n\u001b[32m    233\u001b[39m             path,\n\u001b[32m   (...)\u001b[39m\u001b[32m    238\u001b[39m         )\n\u001b[32m    239\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path, contexts_per_doc, scores_per_doc \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[32m    243\u001b[39m     contexts.extend(contexts_per_doc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:269\u001b[39m, in \u001b[36mContextGenerator._a_process_document_async\u001b[39m\u001b[34m(self, path, num_context_per_source_file, max_context_size, generation_p_bar, source_files_to_collections_map)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_a_process_document_async\u001b[39m(\n\u001b[32m    261\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    262\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    266\u001b[39m     source_files_to_collections_map: Dict,\n\u001b[32m    267\u001b[39m ):\n\u001b[32m    268\u001b[39m     contexts_per_doc, scores_per_doc = (\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._a_get_n_random_contexts_per_source_file(\n\u001b[32m    270\u001b[39m             path=path,\n\u001b[32m    271\u001b[39m             n_contexts_per_source_file=num_context_per_source_file,\n\u001b[32m    272\u001b[39m             context_size=max_context_size,\n\u001b[32m    273\u001b[39m             similarity_threshold=\u001b[38;5;28mself\u001b[39m.similarity_threshold,\n\u001b[32m    274\u001b[39m             generation_p_bar=generation_p_bar,\n\u001b[32m    275\u001b[39m             source_files_to_collections_map=source_files_to_collections_map,\n\u001b[32m    276\u001b[39m         )\n\u001b[32m    277\u001b[39m     )\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m path, contexts_per_doc, scores_per_doc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:385\u001b[39m, in \u001b[36mContextGenerator._a_get_n_random_contexts_per_source_file\u001b[39m\u001b[34m(self, path, n_contexts_per_source_file, context_size, similarity_threshold, generation_p_bar, source_files_to_collections_map)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# Sample n random chunks from each doc (each random chunk is the first chunk in each context)\u001b[39;00m\n\u001b[32m    379\u001b[39m filling_p_bar = tqdm_bar(\n\u001b[32m    380\u001b[39m     total=(context_size - \u001b[32m1\u001b[39m) * n_contexts_per_source_file,\n\u001b[32m    381\u001b[39m     desc=\u001b[33m\"\u001b[39m\u001b[33m  ✨ 🫗 ✨ Filling Contexts\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    382\u001b[39m     leave=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m )\n\u001b[32m    384\u001b[39m random_chunks, scores = (\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._a_get_n_random_chunks_per_source_file(\n\u001b[32m    386\u001b[39m         path,\n\u001b[32m    387\u001b[39m         n_contexts_per_source_file,\n\u001b[32m    388\u001b[39m         generation_p_bar,\n\u001b[32m    389\u001b[39m         source_files_to_collections_map,\n\u001b[32m    390\u001b[39m     )\n\u001b[32m    391\u001b[39m )\n\u001b[32m    393\u001b[39m \u001b[38;5;66;03m# Find similar chunks for each context\u001b[39;00m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m random_chunk \u001b[38;5;129;01min\u001b[39;00m random_chunks:\n\u001b[32m    395\u001b[39m     \u001b[38;5;66;03m# Create context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:528\u001b[39m, in \u001b[36mContextGenerator._a_get_n_random_chunks_per_source_file\u001b[39m\u001b[34m(self, path, n_chunks, p_bar, source_files_to_collections_map)\u001b[39m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m score\n\u001b[32m    527\u001b[39m tasks = [a_evaluate_chunk_and_update(chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m scores = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m    529\u001b[39m chunk_score_pairs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(chunks, scores))\n\u001b[32m    530\u001b[39m chunk_score_pairs.sort(key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m1\u001b[39m], reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:523\u001b[39m, in \u001b[36mContextGenerator._a_get_n_random_chunks_per_source_file.<locals>.a_evaluate_chunk_and_update\u001b[39m\u001b[34m(chunk)\u001b[39m\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34ma_evaluate_chunk_and_update\u001b[39m(chunk):\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m     score = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.a_evaluate_chunk(chunk)\n\u001b[32m    524\u001b[39m     p_bar.update(\u001b[32m1\u001b[39m)\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:582\u001b[39m, in \u001b[36mContextGenerator.a_evaluate_chunk\u001b[39m\u001b[34m(self, chunk)\u001b[39m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    581\u001b[39m     res: ContextScore = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.a_generate(prompt)\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m     data = \u001b[43mtrimAndLoadJson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m     score = (\n\u001b[32m    584\u001b[39m         data[\u001b[33m\"\u001b[39m\u001b[33mclarity\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    585\u001b[39m         + data[\u001b[33m\"\u001b[39m\u001b[33mdepth\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    586\u001b[39m         + data[\u001b[33m\"\u001b[39m\u001b[33mstructure\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    587\u001b[39m         + data[\u001b[33m\"\u001b[39m\u001b[33mrelevance\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    588\u001b[39m     ) / \u001b[32m4\u001b[39m\n\u001b[32m    589\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/metrics/utils.py:279\u001b[39m, in \u001b[36mtrimAndLoadJson\u001b[39m\u001b[34m(input_string, metric)\u001b[39m\n\u001b[32m    277\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    278\u001b[39m         metric.error = error_str\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_str)\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAn unexpected error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Evaluation LLM outputted an invalid JSON. Please use a better evaluation model."
     ]
    }
   ],
   "source": [
    "goldens = await synthesizer.a_generate_goldens_from_docs(  \n",
    "    document_paths=['/mnt/sdb1/PycharmProjects/CODUP/AI-tutor-other/docs/for_golds/Anatomia_cheloveka_1_tom_2-52-57.pdf',\n",
    "                    '/mnt/sdb1/PycharmProjects/CODUP/AI-tutor-other/docs/for_golds/Kapandzhi_-_Pozvonochnik-276-284.pdf'],  \n",
    "    include_expected_output=True,  \n",
    "    max_goldens_per_context=3,  \n",
    "    context_construction_config=context_construction_config  \n",
    ")  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f94c79e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diploma-au9TFMuk-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
