{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "246dc4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Optional, List, Dict, Any\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "from deepeval.models import DeepEvalBaseEmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0404087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfinityEmbeddingModel(DeepEvalBaseEmbeddingModel):\n",
    "    def __init__(self, model_name: str, base_url: str, api_key: Optional[str] = None):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key if api_key is not None else os.getenv(\"OPENAI_API_KEY\", \"123\")\n",
    "        self._sync_client: Optional[OpenAI] = None\n",
    "        self._async_client: Optional[AsyncOpenAI] = None\n",
    "\n",
    "    def load_model(self) -> OpenAI:\n",
    "        if self._sync_client is None:\n",
    "            self._sync_client = OpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "        return self._sync_client\n",
    "\n",
    "    def load_async_model(self) -> AsyncOpenAI:\n",
    "        if self._async_client is None:\n",
    "            self._async_client = AsyncOpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "        return self._async_client\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        client = self.load_model()\n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                model=self.model_name,\n",
    "                input=text\n",
    "            )\n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Error during synchronous Infinity embedding for text '{text[:50]}...': {e}\")\n",
    "            return []\n",
    "\n",
    "    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        client = self.load_model()\n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                model=self.model_name,\n",
    "                input=texts\n",
    "            )\n",
    "            return [data.embedding for data in response.data]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during synchronous Infinity batch embedding: {e}\")\n",
    "            return [[] for _ in texts] \n",
    "\n",
    "    async def a_embed_text(self, text: str) -> List[float]:\n",
    "        client = self.load_async_model()\n",
    "        try:\n",
    "            response = await client.embeddings.create(\n",
    "                model=self.model_name,\n",
    "                input=text\n",
    "            )\n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Error during asynchronous Infinity embedding for text '{text[:50]}...': {e}\")\n",
    "            return []\n",
    "\n",
    "    async def a_embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        client = self.load_async_model()\n",
    "        try:\n",
    "            response = await client.embeddings.create(\n",
    "                model=self.model_name,\n",
    "                input=texts\n",
    "            )\n",
    "            return [data.embedding for data in response.data]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during asynchronous Infinity batch embedding: {e}\")\n",
    "            return [[] for _ in texts]\n",
    "\n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns the name of the embedding model.\n",
    "        \"\"\"\n",
    "        return self.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c88e7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "CogitoLLM = SGlangModel(model_name=\"deepcogito/cogito-v1-preview-llama-8B\", base_url=\"http://127.0.0.1:30000/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa3380e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'m not sure what you mean by \"Ghdt\". Could you please clarify or ask a specific question? I\\'m here to help!'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CogitoLLM.generate(\"Ghdt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3eb8460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Optional, List, Dict, Any\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "\n",
    "class SGlangModel(DeepEvalBaseLLM):\n",
    "    \"\"\"\n",
    "    Generic DeepEval LLM wrapper for models served via an\n",
    "    OpenAI-compatible API (e.g., SGLang, vLLM).\n",
    "    Can attempt to enable model-specific thinking/reasoning modes\n",
    "    based on model_name and removes <think>...</think> tags from responses if they appear.\n",
    "    \"\"\"\n",
    "    # –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è Cogito\n",
    "    COGITO_THINKING_INSTRUCTION = \"Enable deep thinking subroutine.\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model_name: str, # –ò–º—è –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ª–æ–≥–∏–∫–∏\n",
    "                 base_url: str,   # URL —ç–Ω–¥–ø–æ–∏–Ω—Ç–∞ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–≥–æ API\n",
    "                 api_key: Optional[str] = None, # API –∫–ª—é—á (—á–∞—Å—Ç–æ \"EMPTY\" –∏–ª–∏ –Ω–µ –Ω—É–∂–µ–Ω –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö)\n",
    "                 attempt_thinking_mode: bool = False, # –ü—ã—Ç–∞—Ç—å—Å—è –ª–∏ –≤–∫–ª—é—á–∏—Ç—å —Ä–µ–∂–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π?\n",
    "                 cleaning_method: str = \"rsplit\", # –ú–µ—Ç–æ–¥ –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–≥–æ–≤: 'rsplit' –∏–ª–∏ 'regex'\n",
    "                 max_tokens: int = 8192 # –ú–∞–∫—Å. —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initializes the SGlangModel wrapper.\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the model being served (e.g., \"Qwen/Qwen3-30B-A3B\", \"deepcogito/cogito-v1...\").\n",
    "            base_url: The base URL of the OpenAI-compatible API endpoint.\n",
    "            api_key: Optional API key for the endpoint. Defaults to env variable or \"EMPTY\".\n",
    "            attempt_thinking_mode: If True, tries to enable thinking mode based on model_name. Defaults to False.\n",
    "            cleaning_method: Method to use for removing <think> tags ('rsplit' or 'regex'). Defaults to 'rsplit'.\n",
    "            max_tokens: Maximum number of new tokens to generate. Defaults to 8192.\n",
    "        \"\"\"\n",
    "        self.model_name_original = model_name # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è\n",
    "        self.model_name_lower = model_name.lower() # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key if api_key is not None else os.getenv(\"OPENAI_API_KEY\", \"EMPTY\")\n",
    "        self._sync_client: Optional[OpenAI] = None\n",
    "        self._async_client: Optional[AsyncOpenAI] = None\n",
    "        self.attempt_thinking_mode = attempt_thinking_mode\n",
    "        self.cleaning_method = cleaning_method\n",
    "        self.max_tokens_to_generate = max_tokens\n",
    "\n",
    "    def load_model(self) -> OpenAI:\n",
    "        \"\"\"Loads or returns the synchronous OpenAI client.\"\"\"\n",
    "        if self._sync_client is None:\n",
    "            self._sync_client = OpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "        return self._sync_client\n",
    "\n",
    "    def load_async_model(self) -> AsyncOpenAI:\n",
    "        \"\"\"Loads or returns the asynchronous OpenAI client.\"\"\"\n",
    "        if self._async_client is None:\n",
    "            self._async_client = AsyncOpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "        return self._async_client\n",
    "\n",
    "    def _clean_response(self, raw_response: str) -> str:\n",
    "        \"\"\"\n",
    "        Helper function to remove <think>...</think> blocks from the raw response string.\n",
    "        Uses the method specified in self.cleaning_method.\n",
    "        \"\"\"\n",
    "        if not raw_response:\n",
    "            return \"\"\n",
    "\n",
    "        if self.cleaning_method == \"rsplit\":\n",
    "            closing_tag = '</think>'\n",
    "            if closing_tag in raw_response:\n",
    "                # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É —Ç–µ–≥—É –∏ –±–µ—Ä–µ–º –ø—Ä–∞–≤—É—é —á–∞—Å—Ç—å\n",
    "                main_answer = raw_response.rsplit(closing_tag, 1)[-1].strip()\n",
    "            else:\n",
    "                # –ï—Å–ª–∏ —Ç–µ–≥–∞ –Ω–µ—Ç, –ø—Ä–æ—Å—Ç–æ —É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –ø–æ –∫—Ä–∞—è–º\n",
    "                main_answer = raw_response.strip()\n",
    "            return main_answer\n",
    "\n",
    "        elif self.cleaning_method == \"regex\":\n",
    "            # –®–∞–±–ª–æ–Ω –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è <think>...</think> –∏ –ø—Ä–æ–±–µ–ª–æ–≤ –ø–æ—Å–ª–µ\n",
    "            pattern = r'<think>.*?</think>\\s*'\n",
    "            # –ó–∞–º–µ–Ω—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω–æ–µ –Ω–∞ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É, DOTALL –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫\n",
    "            main_answer = re.sub(pattern, '', raw_response, flags=re.DOTALL).strip()\n",
    "            return main_answer\n",
    "        else:\n",
    "            # –ï—Å–ª–∏ –º–µ—Ç–æ–¥ –Ω–µ 'rsplit' –∏ –Ω–µ 'regex', –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å, —É–±—Ä–∞–≤ –ø—Ä–æ–±–µ–ª—ã\n",
    "            print(f\"Warning: Unknown cleaning_method '{self.cleaning_method}'. Returning raw response.\")\n",
    "            return raw_response.strip()\n",
    "\n",
    "    def _prepare_api_call_args(self, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Prepares the messages list and a dictionary of extra API parameters\n",
    "        based on the model name and the attempt_thinking_mode flag.\n",
    "        \"\"\"\n",
    "        # –ë–∞–∑–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π - —Ç–æ–ª—å–∫–æ –ø—Ä–æ–º–ø—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "        messages: List[Dict[str, str]] = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ API (–Ω–∞–ø—Ä–∏–º–µ—Ä, enable_thinking)\n",
    "        api_extra_params: Dict[str, Any] = {}\n",
    "\n",
    "        if self.attempt_thinking_mode:\n",
    "            # --- –õ–æ–≥–∏–∫–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π ---\n",
    "            if \"cogito\" in self.model_name_lower:\n",
    "                # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è Cogito –í –ù–ê–ß–ê–õ–û —Å–ø–∏—Å–∫–∞\n",
    "                messages.insert(0, {\"role\": \"system\", \"content\": self.COGITO_THINKING_INSTRUCTION})\n",
    "                print(f\"Info: Enabling Cogito thinking mode via system prompt for model '{self.model_name_original}'.\")\n",
    "\n",
    "            elif \"qwen3\" in self.model_name_lower:\n",
    "                # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä API –¥–ª—è Qwen3\n",
    "                # –í–ê–ñ–ù–û: –ò–º—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ 'enable_thinking' —è–≤–ª—è–µ—Ç—Å—è –ü–†–ï–î–ü–û–õ–û–ñ–ï–ù–ò–ï–ú.\n",
    "                # –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –≤–∞—à–µ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞ SGLang/vLLM!\n",
    "                # –ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–Ω–µ—Ç –æ—à–∏–±–∫–∞, –≤–æ–∑–º–æ–∂–Ω–æ, –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 'extra_body'.\n",
    "                api_extra_params[\"enable_thinking\"] = True\n",
    "                print(f\"Info: Attempting to enable Qwen3 thinking mode via API parameter for model '{self.model_name_original}'.\")\n",
    "\n",
    "            else:\n",
    "                # –ú–æ–¥–µ–ª—å –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞ –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∂–∏–º–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π\n",
    "                print(f\"Warning: 'attempt_thinking_mode' is True, but no specific handling defined for model '{self.model_name_original}'. Making standard call.\")\n",
    "            # --- –ö–æ–Ω–µ—Ü –ª–æ–≥–∏–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π ---\n",
    "\n",
    "        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–º–∏ —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ –∏ –¥–æ–ø. –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
    "        return {\"messages\": messages, \"api_extra_params\": api_extra_params}\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates a response synchronously.\n",
    "        If attempt_thinking_mode is True, it modifies the request based on the model.\n",
    "        It always cleans the response to remove <think> tags.\n",
    "        \"\"\"\n",
    "        client = self.load_model()\n",
    "        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –≤—ã–∑–æ–≤–∞ API\n",
    "        call_args = self._prepare_api_call_args(prompt)\n",
    "        messages = call_args[\"messages\"]\n",
    "        api_extra_params = call_args[\"api_extra_params\"]\n",
    "\n",
    "        try:\n",
    "            # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—ã–∑–æ–≤ API, –ø–µ—Ä–µ–¥–∞–≤–∞—è –æ—Å–Ω–æ–≤–Ω—ã–µ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model_name_original, # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è –º–æ–¥–µ–ª–∏\n",
    "                messages=messages,\n",
    "                max_tokens=self.max_tokens_to_generate,\n",
    "                **api_extra_params # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –¥–æ–ø. –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º)\n",
    "                # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ï—Å–ª–∏ 'enable_thinking' –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –ø—Ä—è–º–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä,\n",
    "                # –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ–¥–∞—Ç—å –µ–≥–æ —Ç–∞–∫ (–∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–≤ —Å—Ç—Ä–æ–∫—É –≤—ã—à–µ —Å **api_extra_params):\n",
    "                # extra_body=api_extra_params if api_extra_params else None\n",
    "            )\n",
    "            # –ü–æ–ª—É—á–∞–µ–º —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç\n",
    "            raw_response_content = response.choices[0].message.content\n",
    "            # –í—Å–µ–≥–¥–∞ –æ—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç —Ç–µ–≥–æ–≤ <think>\n",
    "            cleaned_response = self._clean_response(raw_response_content)\n",
    "            return cleaned_response\n",
    "        except Exception as e:\n",
    "            print(f\"Error during synchronous generation for model '{self.model_name_original}', prompt '{prompt[:50]}...': {e}\")\n",
    "            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥—Å–∫–∞–∑–∫–∏, –µ—Å–ª–∏ –æ—à–∏–±–∫–∞ —Å–≤—è–∑–∞–Ω–∞ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º 'enable_thinking'\n",
    "            if api_extra_params.get(\"enable_thinking\") and \"unexpected keyword argument 'enable_thinking'\" in str(e).lower():\n",
    "                 print(\"Hint: The API might not support 'enable_thinking' as a direct parameter. Try modifying the wrapper to use 'extra_body'.\")\n",
    "            elif api_extra_params.get(\"enable_thinking\") and \"extra_body\" in str(e).lower():\n",
    "                 print(\"Hint: Check the exact API documentation for SGLang/vLLM OpenAI-compatible endpoint for enabling Qwen3 thinking mode.\")\n",
    "            return \"\" # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates a response asynchronously.\n",
    "        If attempt_thinking_mode is True, it modifies the request based on the model.\n",
    "        It always cleans the response to remove <think> tags.\n",
    "        \"\"\"\n",
    "        client = self.load_async_model()\n",
    "        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –≤—ã–∑–æ–≤–∞ API\n",
    "        call_args = self._prepare_api_call_args(prompt)\n",
    "        messages = call_args[\"messages\"]\n",
    "        api_extra_params = call_args[\"api_extra_params\"]\n",
    "\n",
    "        try:\n",
    "            # –í—ã–ø–æ–ª–Ω—è–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ API\n",
    "            response = await client.chat.completions.create(\n",
    "                model=self.model_name_original, # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è –º–æ–¥–µ–ª–∏\n",
    "                messages=messages,\n",
    "                max_tokens=self.max_tokens_to_generate,\n",
    "                **api_extra_params # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –¥–æ–ø. –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "                 # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ï—Å–ª–∏ 'enable_thinking' –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –ø—Ä—è–º–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä,\n",
    "                 # –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ–¥–∞—Ç—å –µ–≥–æ —Ç–∞–∫ (–∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–≤ —Å—Ç—Ä–æ–∫—É –≤—ã—à–µ —Å **api_extra_params):\n",
    "                 # extra_body=api_extra_params if api_extra_params else None\n",
    "            )\n",
    "            # –ü–æ–ª—É—á–∞–µ–º —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç\n",
    "            raw_response_content = response.choices[0].message.content\n",
    "            # –í—Å–µ–≥–¥–∞ –æ—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç —Ç–µ–≥–æ–≤ <think>\n",
    "            cleaned_response = self._clean_response(raw_response_content)\n",
    "            return cleaned_response\n",
    "        except Exception as e:\n",
    "            print(f\"Error during asynchronous generation for model '{self.model_name_original}', prompt '{prompt[:50]}...': {e}\")\n",
    "            # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –ø—Ä–æ enable_thinking / extra_body\n",
    "            if api_extra_params.get(\"enable_thinking\") and \"unexpected keyword argument 'enable_thinking'\" in str(e).lower():\n",
    "                 print(\"Hint: The API might not support 'enable_thinking' as a direct parameter. Try modifying the wrapper to use 'extra_body'.\")\n",
    "            elif api_extra_params.get(\"enable_thinking\") and \"extra_body\" in str(e).lower():\n",
    "                 print(\"Hint: Check the exact API documentation for SGLang/vLLM OpenAI-compatible endpoint for enabling Qwen3 thinking mode.\")\n",
    "            return \"\" # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏\n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns the original name of the model used for initialization.\n",
    "        \"\"\"\n",
    "        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–µ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "        return self.model_name_original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e8311c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CogitoLLM= SGlangModel(model_name=\"deepcogito/cogito-v1-preview-llama-8B\", base_url=\"http://85.143.167.11:30000/v1\")\n",
    "\n",
    "# Qwen3_30_Reasoning = LLMModel(model_name=\"qwen3-30-lmstudio\", base_url=\"http://85.143.167.11:30000/v1\", attempt_thinking_mode=True, cleaning_method=\"rsplit\")\n",
    "# Qwen3_30 = LLMModel(model_name=\"qwen3-30-lmstudio\", base_url=\"http://85.143.167.11:30000/v1\", attempt_thinking_mode=False)\n",
    "\n",
    "# Qwen3_32_Reasoning = LLMModel(model_name=\"Qwen/Qwen3-32B-AWQ\", base_url=\"http://85.143.167.11:30000/v1\", attempt_thinking_mode=True, cleaning_method=\"rsplit\")\n",
    "# Qwen3_32 = LLMModel(model_name=\"Qwen/Qwen3-32B-AWQ\", base_url=\"http://85.143.167.11:30000/v1\", attempt_thinking_mode=False)\n",
    "\n",
    "# Qwen3_8_Reasoning = LLMModel(model_name=\"Qwen/Qwen3-8B-FP8\", base_url=\"http://85.143.167.11:30000/v1\", attempt_thinking_mode=True, cleaning_method=\"rsplit\")\n",
    "# Qwen3_8 = LLMModel(model_name=\"Qwen/Qwen3-8B-FP8\", base_url=\"http://85.143.167.11:30000/v1\", attempt_thinking_mode=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd754b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "‚ú® üöÄ ‚ú® Loading Documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.62it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Database error: error returned from database: (code: 14) unable to open database file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m context_construction_config = ContextConstructionConfig(embedder = BertaEmbeddings,\n\u001b[32m      2\u001b[39m                                                         critic_model = CogitoLLM,\n\u001b[32m      3\u001b[39m                                                         context_quality_threshold = \u001b[32m0.1\u001b[39m) \n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m data = \u001b[43msynthesizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_goldens_from_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocument_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/home/user/Projects/data-extracting/data/Kapandzhi_-_Pozvonochnik-276-284.pdf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_construction_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_construction_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/synthesizer.py:129\u001b[39m, in \u001b[36mSynthesizer.generate_goldens_from_docs\u001b[39m\u001b[34m(self, document_paths, include_expected_output, max_goldens_per_context, context_construction_config, _send_data)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_mode:\n\u001b[32m    128\u001b[39m     loop = get_or_create_event_loop()\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     goldens = \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ma_generate_goldens_from_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdocument_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocument_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m            \u001b[49m\u001b[43minclude_expected_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_expected_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_goldens_per_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_goldens_per_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontext_construction_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_construction_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_reset_cost\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# Generate contexts from provided docs\u001b[39;00m\n\u001b[32m    140\u001b[39m     context_generator = ContextGenerator(\n\u001b[32m    141\u001b[39m         document_paths=document_paths,\n\u001b[32m    142\u001b[39m         encoding=context_construction_config.encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    149\u001b[39m         max_retries=context_construction_config.max_retries,\n\u001b[32m    150\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/asyncio/futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/asyncio/tasks.py:277\u001b[39m, in \u001b[36mTask.__step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    275\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    276\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    279\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/synthesizer.py:220\u001b[39m, in \u001b[36mSynthesizer.a_generate_goldens_from_docs\u001b[39m\u001b[34m(self, document_paths, include_expected_output, max_goldens_per_context, context_construction_config, _reset_cost)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# Generate contexts from provided docs\u001b[39;00m\n\u001b[32m    208\u001b[39m context_generator = ContextGenerator(\n\u001b[32m    209\u001b[39m     document_paths=document_paths,\n\u001b[32m    210\u001b[39m     encoding=context_construction_config.encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    217\u001b[39m     max_retries=context_construction_config.max_retries,\n\u001b[32m    218\u001b[39m )\n\u001b[32m    219\u001b[39m contexts, source_files, context_scores = (\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m context_generator.a_generate_contexts(\n\u001b[32m    221\u001b[39m         max_contexts_per_source_file=context_construction_config.max_contexts_per_document,\n\u001b[32m    222\u001b[39m         min_contexts_per_source_file=context_construction_config.min_contexts_per_document,\n\u001b[32m    223\u001b[39m         max_context_size=context_construction_config.max_context_length,\n\u001b[32m    224\u001b[39m         min_context_size=context_construction_config.min_context_length,\n\u001b[32m    225\u001b[39m     )\n\u001b[32m    226\u001b[39m )\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.synthesis_cost:\n\u001b[32m    228\u001b[39m     \u001b[38;5;28mself\u001b[39m.synthesis_cost += context_generator.total_cost\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:204\u001b[39m, in \u001b[36mContextGenerator.a_generate_contexts\u001b[39m\u001b[34m(self, max_contexts_per_source_file, min_contexts_per_source_file, max_context_size, min_context_size)\u001b[39m\n\u001b[32m    198\u001b[39m     source_files_to_chunk_collections_map[key] = collection\n\u001b[32m    200\u001b[39m tasks = [\n\u001b[32m    201\u001b[39m     a_chunk_and_store(key, chunker)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, chunker \u001b[38;5;129;01min\u001b[39;00m source_file_to_chunker_map.items()\n\u001b[32m    203\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m tqdm_asyncio.gather(\n\u001b[32m    205\u001b[39m     *tasks, desc=\u001b[33m\"\u001b[39m\u001b[33m‚ú® üìö ‚ú® Chunking Documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    206\u001b[39m )\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# Intialize progress bar for context generation\u001b[39;00m\n\u001b[32m    209\u001b[39m num_contexts = \u001b[38;5;28msum\u001b[39m(\n\u001b[32m    210\u001b[39m     \u001b[38;5;28mmin\u001b[39m(max_contexts_per_source_file, collection.count())\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, collection \u001b[38;5;129;01min\u001b[39;00m source_files_to_chunk_collections_map.items()\n\u001b[32m    212\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/tqdm/asyncio.py:79\u001b[39m, in \u001b[36mtqdm_asyncio.gather\u001b[39m\u001b[34m(cls, loop, timeout, total, *fs, **tqdm_kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n\u001b[32m     78\u001b[39m ifs = [wrap_awaitable(i, f) \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fs)]\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m res = [\u001b[38;5;28;01mawait\u001b[39;00m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.as_completed(ifs, loop=loop, timeout=timeout,\n\u001b[32m     80\u001b[39m                                          total=total, **tqdm_kwargs)]\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m _, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(res)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/tqdm/asyncio.py:79\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n\u001b[32m     78\u001b[39m ifs = [wrap_awaitable(i, f) \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fs)]\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m res = [\u001b[38;5;28;01mawait\u001b[39;00m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.as_completed(ifs, loop=loop, timeout=timeout,\n\u001b[32m     80\u001b[39m                                          total=total, **tqdm_kwargs)]\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m _, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(res)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/asyncio/tasks.py:615\u001b[39m, in \u001b[36mas_completed.<locals>._wait_for_one\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    613\u001b[39m     \u001b[38;5;66;03m# Dummy value from _on_timeout().\u001b[39;00m\n\u001b[32m    614\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.TimeoutError\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/asyncio/futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/asyncio/tasks.py:277\u001b[39m, in \u001b[36mTask.__step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    275\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    276\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    279\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/tqdm/asyncio.py:76\u001b[39m, in \u001b[36mtqdm_asyncio.gather.<locals>.wrap_awaitable\u001b[39m\u001b[34m(i, f)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_awaitable\u001b[39m(i, f):\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:192\u001b[39m, in \u001b[36mContextGenerator.a_generate_contexts.<locals>.a_chunk_and_store\u001b[39m\u001b[34m(key, chunker)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34ma_chunk_and_store\u001b[39m(key, chunker: DocumentChunker):\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     collection = \u001b[38;5;28;01mawait\u001b[39;00m chunker.a_chunk_doc(\n\u001b[32m    193\u001b[39m         \u001b[38;5;28mself\u001b[39m.chunk_size, \u001b[38;5;28mself\u001b[39m.chunk_overlap\n\u001b[32m    194\u001b[39m     )\n\u001b[32m    195\u001b[39m     \u001b[38;5;28mself\u001b[39m.validate_chunk_size(\n\u001b[32m    196\u001b[39m         min_contexts_per_source_file, collection\n\u001b[32m    197\u001b[39m     )\n\u001b[32m    198\u001b[39m     source_files_to_chunk_collections_map[key] = collection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/doc_chunker.py:57\u001b[39m, in \u001b[36mDocumentChunker.a_chunk_doc\u001b[39m\u001b[34m(self, chunk_size, chunk_overlap)\u001b[39m\n\u001b[32m     55\u001b[39m full_document_path, _ = os.path.splitext(\u001b[38;5;28mself\u001b[39m.source_file)\n\u001b[32m     56\u001b[39m document_name = os.path.basename(full_document_path)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m client = \u001b[43mchromadb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPersistentClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.vector_db/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdocument_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m collection_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprocessed_chunks_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_overlap\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/chromadb/__init__.py:161\u001b[39m, in \u001b[36mPersistentClient\u001b[39m\u001b[34m(path, settings, tenant, database)\u001b[39m\n\u001b[32m    158\u001b[39m tenant = \u001b[38;5;28mstr\u001b[39m(tenant)\n\u001b[32m    159\u001b[39m database = \u001b[38;5;28mstr\u001b[39m(database)\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mClientCreator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m=\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/chromadb/api/client.py:85\u001b[39m, in \u001b[36mClient.__init__\u001b[39m\u001b[34m(self, tenant, database, settings)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Create an admin client for verifying that databases and tenants exist\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mself\u001b[39m._admin_client = AdminClient.from_system(\u001b[38;5;28mself\u001b[39m._system)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_tenant_database\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m._submit_client_start_event()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/chromadb/api/client.py:433\u001b[39m, in \u001b[36mClient._validate_tenant_database\u001b[39m\u001b[34m(self, tenant, database)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;66;03m# Propagate ChromaErrors\u001b[39;00m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ChromaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    436\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not connect to tenant \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtenant\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Are you sure it exists?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    437\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/chromadb/api/client.py:426\u001b[39m, in \u001b[36mClient._validate_tenant_database\u001b[39m\u001b[34m(self, tenant, database)\u001b[39m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_tenant_database\u001b[39m(\u001b[38;5;28mself\u001b[39m, tenant: \u001b[38;5;28mstr\u001b[39m, database: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    425\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_admin_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tenant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    427\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m    428\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    429\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCould not connect to a Chroma server. Are you sure it is running?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    430\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/chromadb/api/client.py:483\u001b[39m, in \u001b[36mAdminClient.get_tenant\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    481\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_tenant\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Tenant:\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_server\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tenant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/chromadb/api/rust.py:166\u001b[39m, in \u001b[36mRustBindingsAPI.get_tenant\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_tenant\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Tenant:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m     tenant = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tenant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Tenant(name=tenant.name)\n",
      "\u001b[31mInternalError\u001b[39m: Database error: error returned from database: (code: 14) unable to open database file"
     ]
    }
   ],
   "source": [
    "context_construction_config = ContextConstructionConfig(embedder = BertaEmbeddings,\n",
    "                                                        critic_model = CogitoLLM,\n",
    "                                                        context_quality_threshold = 0.1) \n",
    "\n",
    "data = synthesizer.generate_goldens_from_docs(\n",
    "    document_paths=['/home/user/Projects/data-extracting/data/Kapandzhi_-_Pozvonochnik-276-284.pdf'],\n",
    "    context_construction_config=context_construction_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79b80e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.synthesizer import Synthesizer, Evolution  \n",
    "from deepeval.synthesizer.config import StylingConfig, EvolutionConfig, ContextConstructionConfig, FiltrationConfig  \n",
    "  \n",
    "context_construction_config = ContextConstructionConfig(  \n",
    "    chunk_size=2048,                 \n",
    "    chunk_overlap=0,            \n",
    "    max_contexts_per_document=3,\n",
    "    context_quality_threshold=0.4,\n",
    "    max_retries=2,\n",
    "    critic_model = CogitoLLM,\n",
    "    embedder = BertaEmbeddings)  \n",
    "  \n",
    "styling_config = StylingConfig(  \n",
    "    input_format=\"\"\"–°–ª–æ–∂–Ω—ã–µ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —É—á–µ–±–Ω–∏–∫–æ–≤.   \n",
    "    –í–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–æ—á–Ω–æ–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏,   \n",
    "    —Ç—Ä–µ–±–æ–≤–∞—Ç—å –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–∞ –∏ –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø—Ä–∏–º–µ–Ω—è—Ç—å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞–Ω–∏—è   \n",
    "    –∫ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–º —Å–∏—Ç—É–∞—Ü–∏—è–º.\"\"\",  \n",
    "      \n",
    "    expected_output_format=\"\"\"–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ, –Ω–∞—É—á–Ω–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ,   \n",
    "    –≤–∫–ª—é—á–∞—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ —É—á–µ–±–Ω–∏–∫–∞, —Ç–æ—á–Ω—ã–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –º–µ—Ö–∞–Ω–∏–∑–º—ã.   \n",
    "    –û—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω—ã –≤ –ª–æ–≥–∏—á–µ—Å–∫–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, –≥–¥–µ —ç—Ç–æ —É–º–µ—Å—Ç–Ω–æ.\"\"\",  \n",
    "      \n",
    "    task=\"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —ç–∫–∑–∞–º–µ–Ω–∞—Ü–∏–æ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ,   \n",
    "    –∫–æ—Ç–æ—Ä—ã–µ —Ç–æ—á–Ω–æ –æ—Ç—Ä–∞–∂–∞—é—Ç —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —É—á–µ–±–Ω–∏–∫–∞ –∏ –ø—Ä–æ–≤–µ—Ä—è—é—Ç –≥–ª—É–±–∏–Ω—É –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–∞.\"\"\",  \n",
    "      \n",
    "    scenario=\"\"\"–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –≤—É–∑–∞ —Å–æ–∑–¥–∞–µ—Ç –±–∞–Ω–∫ –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è —ç–∫–∑–∞–º–µ–Ω–æ–≤,   \n",
    "    —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤. –í–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã —Ç–æ—á–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å   \n",
    "    —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é —É—á–µ–±–Ω–∏–∫–æ–≤ –∏ –±—ã—Ç—å –ø—Ä–∏–≥–æ–¥–Ω—ã–º–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π —Å—Ç—É–¥–µ–Ω—Ç–æ–≤   \n",
    "    —Ä–∞–∑–Ω—ã—Ö –∫—É—Ä—Å–æ–≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è.\"\"\",  \n",
    ")  \n",
    "  \n",
    "evolution_config = EvolutionConfig(  \n",
    "    num_evolutions=2, \n",
    "    evolutions={  \n",
    "        Evolution.CONCRETIZING: 0.25,     \n",
    "        Evolution.MULTICONTEXT: 0.25,   \n",
    "        Evolution.COMPARATIVE: 0.25,   \n",
    "        Evolution.CONSTRAINED: 0.25,\n",
    "    }  \n",
    ") \n",
    "\n",
    "filtration_config = FiltrationConfig(  \n",
    "    synthetic_input_quality_threshold=0.5,\n",
    "    critic_model= CogitoLLM,\n",
    "    max_quality_retries=2                \n",
    ")\n",
    "\n",
    "synthesizer = Synthesizer(\n",
    "    model=CogitoLLM,  \n",
    "    styling_config=styling_config,  \n",
    "    # evolution_config=evolution_config,\n",
    "    # filtration_config=filtration_config,  \n",
    "    async_mode=True,  \n",
    "    max_concurrent=8\n",
    ")  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f92b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ú® üöÄ ‚ú® Loading Documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.00s/it]\n",
      "‚ú® üìö ‚ú® Chunking Documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.03it/s]\n",
      "‚ú® üß© ‚ú® Generating Contexts:   0%|          | 0/12 [00:00<?, ?it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Evaluation LLM outputted an invalid JSON. Please use a better evaluation model.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/metrics/utils.py:274\u001b[39m, in \u001b[36mtrimAndLoadJson\u001b[39m\u001b[34m(input_string, metric)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjsonStr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m json.JSONDecodeError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/json/decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m end = _w(s, end).end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/json/decoder.py:355\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m goldens = \u001b[38;5;28;01mawait\u001b[39;00m synthesizer.a_generate_goldens_from_docs(  \n\u001b[32m      2\u001b[39m     document_paths=[\u001b[33m'\u001b[39m\u001b[33m/mnt/sdb1/PycharmProjects/CODUP/AI-tutor-other/docs/for_golds/Anatomia_cheloveka_1_tom_2-52-57.pdf\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      3\u001b[39m                     \u001b[33m'\u001b[39m\u001b[33m/mnt/sdb1/PycharmProjects/CODUP/AI-tutor-other/docs/for_golds/Kapandzhi_-_Pozvonochnik-276-284.pdf\u001b[39m\u001b[33m'\u001b[39m],  \n\u001b[32m      4\u001b[39m     include_expected_output=\u001b[38;5;28;01mTrue\u001b[39;00m,  \n\u001b[32m      5\u001b[39m     max_goldens_per_context=\u001b[32m3\u001b[39m,  \n\u001b[32m      6\u001b[39m     context_construction_config=context_construction_config  \n\u001b[32m      7\u001b[39m )  \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/synthesizer.py:220\u001b[39m, in \u001b[36mSynthesizer.a_generate_goldens_from_docs\u001b[39m\u001b[34m(self, document_paths, include_expected_output, max_goldens_per_context, context_construction_config, _reset_cost)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# Generate contexts from provided docs\u001b[39;00m\n\u001b[32m    208\u001b[39m context_generator = ContextGenerator(\n\u001b[32m    209\u001b[39m     document_paths=document_paths,\n\u001b[32m    210\u001b[39m     encoding=context_construction_config.encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    217\u001b[39m     max_retries=context_construction_config.max_retries,\n\u001b[32m    218\u001b[39m )\n\u001b[32m    219\u001b[39m contexts, source_files, context_scores = (\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m context_generator.a_generate_contexts(\n\u001b[32m    221\u001b[39m         max_contexts_per_source_file=context_construction_config.max_contexts_per_document,\n\u001b[32m    222\u001b[39m         min_contexts_per_source_file=context_construction_config.min_contexts_per_document,\n\u001b[32m    223\u001b[39m         max_context_size=context_construction_config.max_context_length,\n\u001b[32m    224\u001b[39m         min_context_size=context_construction_config.min_context_length,\n\u001b[32m    225\u001b[39m     )\n\u001b[32m    226\u001b[39m )\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.synthesis_cost:\n\u001b[32m    228\u001b[39m     \u001b[38;5;28mself\u001b[39m.synthesis_cost += context_generator.total_cost\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:241\u001b[39m, in \u001b[36mContextGenerator.a_generate_contexts\u001b[39m\u001b[34m(self, max_contexts_per_source_file, min_contexts_per_source_file, max_context_size, min_context_size)\u001b[39m\n\u001b[32m    230\u001b[39m     max_context_size = \u001b[38;5;28mmin\u001b[39m(max_context_size, collection.count())\n\u001b[32m    231\u001b[39m     tasks.append(\n\u001b[32m    232\u001b[39m         \u001b[38;5;28mself\u001b[39m._a_process_document_async(\n\u001b[32m    233\u001b[39m             path,\n\u001b[32m   (...)\u001b[39m\u001b[32m    238\u001b[39m         )\n\u001b[32m    239\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path, contexts_per_doc, scores_per_doc \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[32m    243\u001b[39m     contexts.extend(contexts_per_doc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:269\u001b[39m, in \u001b[36mContextGenerator._a_process_document_async\u001b[39m\u001b[34m(self, path, num_context_per_source_file, max_context_size, generation_p_bar, source_files_to_collections_map)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_a_process_document_async\u001b[39m(\n\u001b[32m    261\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    262\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    266\u001b[39m     source_files_to_collections_map: Dict,\n\u001b[32m    267\u001b[39m ):\n\u001b[32m    268\u001b[39m     contexts_per_doc, scores_per_doc = (\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._a_get_n_random_contexts_per_source_file(\n\u001b[32m    270\u001b[39m             path=path,\n\u001b[32m    271\u001b[39m             n_contexts_per_source_file=num_context_per_source_file,\n\u001b[32m    272\u001b[39m             context_size=max_context_size,\n\u001b[32m    273\u001b[39m             similarity_threshold=\u001b[38;5;28mself\u001b[39m.similarity_threshold,\n\u001b[32m    274\u001b[39m             generation_p_bar=generation_p_bar,\n\u001b[32m    275\u001b[39m             source_files_to_collections_map=source_files_to_collections_map,\n\u001b[32m    276\u001b[39m         )\n\u001b[32m    277\u001b[39m     )\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m path, contexts_per_doc, scores_per_doc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:385\u001b[39m, in \u001b[36mContextGenerator._a_get_n_random_contexts_per_source_file\u001b[39m\u001b[34m(self, path, n_contexts_per_source_file, context_size, similarity_threshold, generation_p_bar, source_files_to_collections_map)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# Sample n random chunks from each doc (each random chunk is the first chunk in each context)\u001b[39;00m\n\u001b[32m    379\u001b[39m filling_p_bar = tqdm_bar(\n\u001b[32m    380\u001b[39m     total=(context_size - \u001b[32m1\u001b[39m) * n_contexts_per_source_file,\n\u001b[32m    381\u001b[39m     desc=\u001b[33m\"\u001b[39m\u001b[33m  ‚ú® ü´ó ‚ú® Filling Contexts\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    382\u001b[39m     leave=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m )\n\u001b[32m    384\u001b[39m random_chunks, scores = (\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._a_get_n_random_chunks_per_source_file(\n\u001b[32m    386\u001b[39m         path,\n\u001b[32m    387\u001b[39m         n_contexts_per_source_file,\n\u001b[32m    388\u001b[39m         generation_p_bar,\n\u001b[32m    389\u001b[39m         source_files_to_collections_map,\n\u001b[32m    390\u001b[39m     )\n\u001b[32m    391\u001b[39m )\n\u001b[32m    393\u001b[39m \u001b[38;5;66;03m# Find similar chunks for each context\u001b[39;00m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m random_chunk \u001b[38;5;129;01min\u001b[39;00m random_chunks:\n\u001b[32m    395\u001b[39m     \u001b[38;5;66;03m# Create context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:528\u001b[39m, in \u001b[36mContextGenerator._a_get_n_random_chunks_per_source_file\u001b[39m\u001b[34m(self, path, n_chunks, p_bar, source_files_to_collections_map)\u001b[39m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m score\n\u001b[32m    527\u001b[39m tasks = [a_evaluate_chunk_and_update(chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m scores = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m    529\u001b[39m chunk_score_pairs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(chunks, scores))\n\u001b[32m    530\u001b[39m chunk_score_pairs.sort(key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m1\u001b[39m], reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:523\u001b[39m, in \u001b[36mContextGenerator._a_get_n_random_chunks_per_source_file.<locals>.a_evaluate_chunk_and_update\u001b[39m\u001b[34m(chunk)\u001b[39m\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34ma_evaluate_chunk_and_update\u001b[39m(chunk):\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m     score = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.a_evaluate_chunk(chunk)\n\u001b[32m    524\u001b[39m     p_bar.update(\u001b[32m1\u001b[39m)\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:582\u001b[39m, in \u001b[36mContextGenerator.a_evaluate_chunk\u001b[39m\u001b[34m(self, chunk)\u001b[39m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    581\u001b[39m     res: ContextScore = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.a_generate(prompt)\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m     data = \u001b[43mtrimAndLoadJson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m     score = (\n\u001b[32m    584\u001b[39m         data[\u001b[33m\"\u001b[39m\u001b[33mclarity\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    585\u001b[39m         + data[\u001b[33m\"\u001b[39m\u001b[33mdepth\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    586\u001b[39m         + data[\u001b[33m\"\u001b[39m\u001b[33mstructure\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    587\u001b[39m         + data[\u001b[33m\"\u001b[39m\u001b[33mrelevance\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    588\u001b[39m     ) / \u001b[32m4\u001b[39m\n\u001b[32m    589\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/metrics/utils.py:279\u001b[39m, in \u001b[36mtrimAndLoadJson\u001b[39m\u001b[34m(input_string, metric)\u001b[39m\n\u001b[32m    277\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    278\u001b[39m         metric.error = error_str\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_str)\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAn unexpected error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Evaluation LLM outputted an invalid JSON. Please use a better evaluation model."
     ]
    }
   ],
   "source": [
    "goldens = await synthesizer.a_generate_goldens_from_docs(  \n",
    "    document_paths=['/mnt/sdb1/PycharmProjects/CODUP/AI-tutor-other/docs/for_golds/Anatomia_cheloveka_1_tom_2-52-57.pdf',\n",
    "                    '/mnt/sdb1/PycharmProjects/CODUP/AI-tutor-other/docs/for_golds/Kapandzhi_-_Pozvonochnik-276-284.pdf'],  \n",
    "    include_expected_output=True,  \n",
    "    max_goldens_per_context=3,  \n",
    "    context_construction_config=context_construction_config  \n",
    ")  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f94c79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83051093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deepeval.dataset import EvaluationDataset  \n",
    "# dataset = EvaluationDataset(goldens=goldens)  \n",
    "# dataset.push(alias=\"–ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã –∏–∑ —É—á–µ–±–Ω–∏–∫–æ–≤\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diploma-au9TFMuk-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
