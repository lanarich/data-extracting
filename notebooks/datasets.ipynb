{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "246dc4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "\n",
    "class SGlangModel(DeepEvalBaseLLM):\n",
    "    \"\"\"\n",
    "    Custom DeepEval LLM wrapper for a model served via vLLM\n",
    "    with an OpenAI-compatible API.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, base_url: str, api_key: Optional[str] = None):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key if api_key is not None else os.getenv(\"OPENAI_API_KEY\", \"123\")\n",
    "        self._sync_client: Optional[OpenAI] = None\n",
    "        self._async_client: Optional[AsyncOpenAI] = None\n",
    "\n",
    "    def load_model(self) -> OpenAI:\n",
    "        if self._sync_client is None:\n",
    "            self._sync_client = OpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "        return self._sync_client\n",
    "        \n",
    "    def load_async_model(self) -> AsyncOpenAI:\n",
    "         if self._async_client is None:\n",
    "             self._async_client = AsyncOpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "         return self._async_client\n",
    "\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        client = self.load_model()\n",
    "        try:\n",
    "            # –í—ã–ø–æ–ª–Ω—è–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ –∫ —ç–Ω–¥–ø–æ–∏–Ω—Ç—É vLLM\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=150 \n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error during synchronous VLLM generation for prompt '{prompt[:50]}...': {e}\")\n",
    "            return \"\" \n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        client = self.load_async_model()\n",
    "        try:\n",
    "            response = await client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=150 \n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error during asynchronous VLLM generation for prompt '{prompt[:50]}...': {e}\")\n",
    "            return \"\" \n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns the name of the model.\n",
    "        \"\"\"\n",
    "        return self.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0404087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.models import DeepEvalBaseEmbeddingModel\n",
    "from typing import List\n",
    "\n",
    "class InfinityEmbeddingModel(DeepEvalBaseEmbeddingModel):\n",
    "    def __init__(self, model_name: str, base_url: str, api_key: Optional[str] = None):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key if api_key is not None else os.getenv(\"OPENAI_API_KEY\", \"123\")\n",
    "        self._sync_client: Optional[OpenAI] = None\n",
    "        self._async_client: Optional[AsyncOpenAI] = None\n",
    "\n",
    "    def load_model(self) -> OpenAI:\n",
    "        if self._sync_client is None:\n",
    "            self._sync_client = OpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "        return self._sync_client\n",
    "\n",
    "    def load_async_model(self) -> AsyncOpenAI:\n",
    "        if self._async_client is None:\n",
    "            self._async_client = AsyncOpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "        return self._async_client\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        client = self.load_model()\n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                model=self.model_name,\n",
    "                input=text\n",
    "            )\n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Error during synchronous Infinity embedding for text '{text[:50]}...': {e}\")\n",
    "            return []\n",
    "\n",
    "    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        client = self.load_model()\n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                model=self.model_name,\n",
    "                input=texts\n",
    "            )\n",
    "            return [data.embedding for data in response.data]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during synchronous Infinity batch embedding: {e}\")\n",
    "            return [[] for _ in texts] \n",
    "\n",
    "    async def a_embed_text(self, text: str) -> List[float]:\n",
    "        client = self.load_async_model()\n",
    "        try:\n",
    "            response = await client.embeddings.create(\n",
    "                model=self.model_name,\n",
    "                input=text\n",
    "            )\n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Error during asynchronous Infinity embedding for text '{text[:50]}...': {e}\")\n",
    "            return []\n",
    "\n",
    "    async def a_embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        client = self.load_async_model()\n",
    "        try:\n",
    "            response = await client.embeddings.create(\n",
    "                model=self.model_name,\n",
    "                input=texts\n",
    "            )\n",
    "            return [data.embedding for data in response.data]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during asynchronous Infinity batch embedding: {e}\")\n",
    "            return [[] for _ in texts]\n",
    "\n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns the name of the embedding model.\n",
    "        \"\"\"\n",
    "        return self.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b516f404",
   "metadata": {},
   "source": [
    "## deepcogito/cogito-v1-preview-llama-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c88e7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "CogitoLLM = SGlangModel(model_name=\"deepcogito/cogito-v1-preview-llama-8B\", base_url=\"http://85.143.167.11:30000/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb8460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BertaEmbeddings = InfinityEmbeddingModel(model_name=\"sergeyzh/BERTA\", base_url=\"http://127.0.0.1:7997\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e8311c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.synthesizer import Synthesizer\n",
    "from deepeval.synthesizer.config import StylingConfig\n",
    "from deepeval.synthesizer.config import ContextConstructionConfig\n",
    "\n",
    "styling_config = StylingConfig(\n",
    "  input_format=\"–í–æ–ø—Ä–æ—Å—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —É—á–µ–±–Ω–∏–∫–æ–≤.\",\n",
    "  task=\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–Ω–∞–Ω–∏–π –∏–ª–∏ –∏–∑—É—á–µ–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —É—á–µ–±–Ω–∏–∫–æ–≤.\",\n",
    "  scenario=\"–°—Ü–µ–Ω–∞—Ä–∏–π: —Å—Ç—É–¥–µ–Ω—Ç –∏–ª–∏ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–º–∏ —É—á–µ–±–Ω–∏–∫–∞–º–∏ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤.\",\n",
    ")\n",
    "\n",
    "synthesizer = Synthesizer(model = CogitoLLM,\n",
    "                          styling_config = styling_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd754b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ú® üöÄ ‚ú® Loading Documents:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ú® üöÄ ‚ú® Loading Documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.15s/it]\n",
      "‚ú® üìö ‚ú® Chunking Documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.15s/it]\n",
      "‚ú® üß© ‚ú® Generating Contexts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.82it/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 39] Directory not empty: 'da1f2d76-4c57-4417-8605-3ab5848ff81d'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m context_construction_config = ContextConstructionConfig(embedder = BertaEmbeddings,\n\u001b[32m      2\u001b[39m                                                         critic_model = CogitoLLM,\n\u001b[32m      3\u001b[39m                                                         context_quality_threshold = \u001b[32m0.1\u001b[39m) \n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m data = \u001b[43msynthesizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_goldens_from_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocument_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/mnt/sdb1/PycharmProjects/CODUP/AI-tutor-other/docs/books/Kapandzhi_-_Pozvonochnik-276-284.pdf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_construction_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_construction_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-UpMma5R9-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/synthesizer.py:129\u001b[39m, in \u001b[36mSynthesizer.generate_goldens_from_docs\u001b[39m\u001b[34m(self, document_paths, include_expected_output, max_goldens_per_context, context_construction_config, _send_data)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_mode:\n\u001b[32m    128\u001b[39m     loop = get_or_create_event_loop()\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     goldens = \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ma_generate_goldens_from_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdocument_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocument_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m            \u001b[49m\u001b[43minclude_expected_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_expected_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_goldens_per_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_goldens_per_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontext_construction_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_construction_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_reset_cost\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# Generate contexts from provided docs\u001b[39;00m\n\u001b[32m    140\u001b[39m     context_generator = ContextGenerator(\n\u001b[32m    141\u001b[39m         document_paths=document_paths,\n\u001b[32m    142\u001b[39m         encoding=context_construction_config.encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    149\u001b[39m         max_retries=context_construction_config.max_retries,\n\u001b[32m    150\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-UpMma5R9-py3.11/lib/python3.11/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/asyncio/futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/asyncio/tasks.py:277\u001b[39m, in \u001b[36mTask.__step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    275\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    276\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    279\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-UpMma5R9-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/synthesizer.py:220\u001b[39m, in \u001b[36mSynthesizer.a_generate_goldens_from_docs\u001b[39m\u001b[34m(self, document_paths, include_expected_output, max_goldens_per_context, context_construction_config, _reset_cost)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# Generate contexts from provided docs\u001b[39;00m\n\u001b[32m    208\u001b[39m context_generator = ContextGenerator(\n\u001b[32m    209\u001b[39m     document_paths=document_paths,\n\u001b[32m    210\u001b[39m     encoding=context_construction_config.encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    217\u001b[39m     max_retries=context_construction_config.max_retries,\n\u001b[32m    218\u001b[39m )\n\u001b[32m    219\u001b[39m contexts, source_files, context_scores = (\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m context_generator.a_generate_contexts(\n\u001b[32m    221\u001b[39m         max_contexts_per_source_file=context_construction_config.max_contexts_per_document,\n\u001b[32m    222\u001b[39m         min_contexts_per_source_file=context_construction_config.min_contexts_per_document,\n\u001b[32m    223\u001b[39m         max_context_size=context_construction_config.max_context_length,\n\u001b[32m    224\u001b[39m         min_context_size=context_construction_config.min_context_length,\n\u001b[32m    225\u001b[39m     )\n\u001b[32m    226\u001b[39m )\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.synthesis_cost:\n\u001b[32m    228\u001b[39m     \u001b[38;5;28mself\u001b[39m.synthesis_cost += context_generator.total_cost\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-UpMma5R9-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:258\u001b[39m, in \u001b[36mContextGenerator.a_generate_contexts\u001b[39m\u001b[34m(self, max_contexts_per_source_file, min_contexts_per_source_file, max_context_size, min_context_size)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    257\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(vector_db_path):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m         \u001b[43mshutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector_db_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/shutil.py:752\u001b[39m, in \u001b[36mrmtree\u001b[39m\u001b[34m(path, ignore_errors, onerror, dir_fd)\u001b[39m\n\u001b[32m    750\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    751\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.samestat(orig_st, os.fstat(fd)):\n\u001b[32m--> \u001b[39m\u001b[32m752\u001b[39m         \u001b[43m_rmtree_safe_fd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    753\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    754\u001b[39m             os.close(fd)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/shutil.py:672\u001b[39m, in \u001b[36m_rmtree_safe_fd\u001b[39m\u001b[34m(topfd, path, onerror)\u001b[39m\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.samestat(orig_st, os.fstat(dirfd)):\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m         \u001b[43m_rmtree_safe_fd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    673\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    674\u001b[39m             os.close(dirfd)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/shutil.py:683\u001b[39m, in \u001b[36m_rmtree_safe_fd\u001b[39m\u001b[34m(topfd, path, onerror)\u001b[39m\n\u001b[32m    681\u001b[39m         os.rmdir(entry.name, dir_fd=topfd)\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m683\u001b[39m         \u001b[43monerror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrmdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    685\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    686\u001b[39m         \u001b[38;5;66;03m# This can only happen if someone replaces\u001b[39;00m\n\u001b[32m    687\u001b[39m         \u001b[38;5;66;03m# a directory with a symlink after the call to\u001b[39;00m\n\u001b[32m    688\u001b[39m         \u001b[38;5;66;03m# os.scandir or stat.S_ISDIR above.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/shutil.py:681\u001b[39m, in \u001b[36m_rmtree_safe_fd\u001b[39m\u001b[34m(topfd, path, onerror)\u001b[39m\n\u001b[32m    679\u001b[39m dirfd_closed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     os.rmdir(entry.name, dir_fd=topfd)\n\u001b[32m    682\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m    683\u001b[39m     onerror(os.rmdir, fullname, sys.exc_info())\n",
      "\u001b[31mOSError\u001b[39m: [Errno 39] Directory not empty: 'da1f2d76-4c57-4417-8605-3ab5848ff81d'"
     ]
    }
   ],
   "source": [
    "context_construction_config = ContextConstructionConfig(embedder = BertaEmbeddings,\n",
    "                                                        critic_model = CogitoLLM,\n",
    "                                                        context_quality_threshold = 0.1) \n",
    "\n",
    "synthesizer.generate_goldens_from_docs(\n",
    "    document_paths=['/mnt/sdb1/PycharmProjects/CODUP/AI-tutor-other/docs/books/Kapandzhi_-_Pozvonochnik-276-284.pdf'],\n",
    "    context_construction_config=context_construction_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62f605c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdata\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diploma-UpMma5R9-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
