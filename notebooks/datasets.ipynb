{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "246dc4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Optional, List, Dict, Any\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "from deepeval.models import DeepEvalBaseEmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0404087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfinityEmbeddingModel(DeepEvalBaseEmbeddingModel):\n",
    "    def __init__(self, model_name: str, base_url: str, api_key: Optional[str] = None):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key if api_key is not None else os.getenv(\"OPENAI_API_KEY\", \"123\")\n",
    "        self._sync_client: Optional[OpenAI] = None\n",
    "        self._async_client: Optional[AsyncOpenAI] = None\n",
    "\n",
    "    def load_model(self) -> OpenAI:\n",
    "        if self._sync_client is None:\n",
    "            self._sync_client = OpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "        return self._sync_client\n",
    "\n",
    "    def load_async_model(self) -> AsyncOpenAI:\n",
    "        if self._async_client is None:\n",
    "            self._async_client = AsyncOpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "        return self._async_client\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        client = self.load_model()\n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                model=self.model_name,\n",
    "                input=text\n",
    "            )\n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Error during synchronous Infinity embedding for text '{text[:50]}...': {e}\")\n",
    "            return []\n",
    "\n",
    "    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        client = self.load_model()\n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                model=self.model_name,\n",
    "                input=texts\n",
    "            )\n",
    "            return [data.embedding for data in response.data]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during synchronous Infinity batch embedding: {e}\")\n",
    "            return [[] for _ in texts] \n",
    "\n",
    "    async def a_embed_text(self, text: str) -> List[float]:\n",
    "        client = self.load_async_model()\n",
    "        try:\n",
    "            response = await client.embeddings.create(\n",
    "                model=self.model_name,\n",
    "                input=text\n",
    "            )\n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Error during asynchronous Infinity embedding for text '{text[:50]}...': {e}\")\n",
    "            return []\n",
    "\n",
    "    async def a_embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        client = self.load_async_model()\n",
    "        try:\n",
    "            response = await client.embeddings.create(\n",
    "                model=self.model_name,\n",
    "                input=texts\n",
    "            )\n",
    "            return [data.embedding for data in response.data]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during asynchronous Infinity batch embedding: {e}\")\n",
    "            return [[] for _ in texts]\n",
    "\n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns the name of the embedding model.\n",
    "        \"\"\"\n",
    "        return self.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c88e7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "CogitoLLM = SGlangModel(model_name=\"deepcogito/cogito-v1-preview-llama-8B\", base_url=\"http://127.0.0.1:30000/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa3380e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'m not sure what you mean by \"Ghdt\". Could you please clarify or ask a specific question? I\\'m here to help!'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CogitoLLM.generate(\"Ghdt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3eb8460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Optional, List, Dict, Any\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "\n",
    "class SGlangModel(DeepEvalBaseLLM):\n",
    "    \"\"\"\n",
    "    Generic DeepEval LLM wrapper for models served via an\n",
    "    OpenAI-compatible API (e.g., SGLang, vLLM).\n",
    "    Can attempt to enable model-specific thinking/reasoning modes\n",
    "    based on model_name and removes <think>...</think> tags from responses if they appear.\n",
    "    \"\"\"\n",
    "    # Специфическая инструкция для Cogito\n",
    "    COGITO_THINKING_INSTRUCTION = \"Enable deep thinking subroutine.\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model_name: str, # Имя модели, используется для логики\n",
    "                 base_url: str,   # URL эндпоинта OpenAI-совместимого API\n",
    "                 api_key: Optional[str] = None, # API ключ (часто \"EMPTY\" или не нужен для локальных)\n",
    "                 attempt_thinking_mode: bool = False, # Пытаться ли включить режим рассуждений?\n",
    "                 cleaning_method: str = \"rsplit\", # Метод очистки тегов: 'rsplit' или 'regex'\n",
    "                 max_tokens: int = 8192 # Макс. токенов для генерации\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initializes the SGlangModel wrapper.\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the model being served (e.g., \"Qwen/Qwen3-30B-A3B\", \"deepcogito/cogito-v1...\").\n",
    "            base_url: The base URL of the OpenAI-compatible API endpoint.\n",
    "            api_key: Optional API key for the endpoint. Defaults to env variable or \"EMPTY\".\n",
    "            attempt_thinking_mode: If True, tries to enable thinking mode based on model_name. Defaults to False.\n",
    "            cleaning_method: Method to use for removing <think> tags ('rsplit' or 'regex'). Defaults to 'rsplit'.\n",
    "            max_tokens: Maximum number of new tokens to generate. Defaults to 8192.\n",
    "        \"\"\"\n",
    "        self.model_name_original = model_name # Сохраняем оригинальное имя\n",
    "        self.model_name_lower = model_name.lower() # Сохраняем в нижнем регистре для сравнения\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key if api_key is not None else os.getenv(\"OPENAI_API_KEY\", \"EMPTY\")\n",
    "        self._sync_client: Optional[OpenAI] = None\n",
    "        self._async_client: Optional[AsyncOpenAI] = None\n",
    "        self.attempt_thinking_mode = attempt_thinking_mode\n",
    "        self.cleaning_method = cleaning_method\n",
    "        self.max_tokens_to_generate = max_tokens\n",
    "\n",
    "    def load_model(self) -> OpenAI:\n",
    "        \"\"\"Loads or returns the synchronous OpenAI client.\"\"\"\n",
    "        if self._sync_client is None:\n",
    "            self._sync_client = OpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "        return self._sync_client\n",
    "\n",
    "    def load_async_model(self) -> AsyncOpenAI:\n",
    "        \"\"\"Loads or returns the asynchronous OpenAI client.\"\"\"\n",
    "        if self._async_client is None:\n",
    "            self._async_client = AsyncOpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "        return self._async_client\n",
    "\n",
    "    def _clean_response(self, raw_response: str) -> str:\n",
    "        \"\"\"\n",
    "        Helper function to remove <think>...</think> blocks from the raw response string.\n",
    "        Uses the method specified in self.cleaning_method.\n",
    "        \"\"\"\n",
    "        if not raw_response:\n",
    "            return \"\"\n",
    "\n",
    "        if self.cleaning_method == \"rsplit\":\n",
    "            closing_tag = '</think>'\n",
    "            if closing_tag in raw_response:\n",
    "                # Разделяем по последнему тегу и берем правую часть\n",
    "                main_answer = raw_response.rsplit(closing_tag, 1)[-1].strip()\n",
    "            else:\n",
    "                # Если тега нет, просто убираем пробелы по краям\n",
    "                main_answer = raw_response.strip()\n",
    "            return main_answer\n",
    "\n",
    "        elif self.cleaning_method == \"regex\":\n",
    "            # Шаблон для удаления <think>...</think> и пробелов после\n",
    "            pattern = r'<think>.*?</think>\\s*'\n",
    "            # Заменяем найденное на пустую строку, DOTALL для переносов строк\n",
    "            main_answer = re.sub(pattern, '', raw_response, flags=re.DOTALL).strip()\n",
    "            return main_answer\n",
    "        else:\n",
    "            # Если метод не 'rsplit' и не 'regex', возвращаем как есть, убрав пробелы\n",
    "            print(f\"Warning: Unknown cleaning_method '{self.cleaning_method}'. Returning raw response.\")\n",
    "            return raw_response.strip()\n",
    "\n",
    "    def _prepare_api_call_args(self, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Prepares the messages list and a dictionary of extra API parameters\n",
    "        based on the model name and the attempt_thinking_mode flag.\n",
    "        \"\"\"\n",
    "        # Базовый список сообщений - только промпт пользователя\n",
    "        messages: List[Dict[str, str]] = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        # Словарь для дополнительных параметров API (например, enable_thinking)\n",
    "        api_extra_params: Dict[str, Any] = {}\n",
    "\n",
    "        if self.attempt_thinking_mode:\n",
    "            # --- Логика для конкретных моделей ---\n",
    "            if \"cogito\" in self.model_name_lower:\n",
    "                # Добавляем системный промпт для Cogito В НАЧАЛО списка\n",
    "                messages.insert(0, {\"role\": \"system\", \"content\": self.COGITO_THINKING_INSTRUCTION})\n",
    "                print(f\"Info: Enabling Cogito thinking mode via system prompt for model '{self.model_name_original}'.\")\n",
    "\n",
    "            elif \"qwen3\" in self.model_name_lower:\n",
    "                # Добавляем параметр API для Qwen3\n",
    "                # ВАЖНО: Имя параметра 'enable_thinking' является ПРЕДПОЛОЖЕНИЕМ.\n",
    "                # Проверьте документацию вашего сервера SGLang/vLLM!\n",
    "                # Если возникнет ошибка, возможно, потребуется использовать 'extra_body'.\n",
    "                api_extra_params[\"enable_thinking\"] = True\n",
    "                print(f\"Info: Attempting to enable Qwen3 thinking mode via API parameter for model '{self.model_name_original}'.\")\n",
    "\n",
    "            else:\n",
    "                # Модель не распознана для специальной обработки режима рассуждений\n",
    "                print(f\"Warning: 'attempt_thinking_mode' is True, but no specific handling defined for model '{self.model_name_original}'. Making standard call.\")\n",
    "            # --- Конец логики для моделей ---\n",
    "\n",
    "        # Возвращаем словарь с подготовленными сообщениями и доп. параметрами\n",
    "        return {\"messages\": messages, \"api_extra_params\": api_extra_params}\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates a response synchronously.\n",
    "        If attempt_thinking_mode is True, it modifies the request based on the model.\n",
    "        It always cleans the response to remove <think> tags.\n",
    "        \"\"\"\n",
    "        client = self.load_model()\n",
    "        # Подготавливаем аргументы для вызова API\n",
    "        call_args = self._prepare_api_call_args(prompt)\n",
    "        messages = call_args[\"messages\"]\n",
    "        api_extra_params = call_args[\"api_extra_params\"]\n",
    "\n",
    "        try:\n",
    "            # Выполняем вызов API, передавая основные и дополнительные параметры\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model_name_original, # Используем оригинальное имя модели\n",
    "                messages=messages,\n",
    "                max_tokens=self.max_tokens_to_generate,\n",
    "                **api_extra_params # Распаковываем доп. параметры (может быть пустым)\n",
    "                # Примечание: Если 'enable_thinking' не работает как прямой параметр,\n",
    "                # попробуйте передать его так (закомментировав строку выше с **api_extra_params):\n",
    "                # extra_body=api_extra_params if api_extra_params else None\n",
    "            )\n",
    "            # Получаем сырой текстовый ответ\n",
    "            raw_response_content = response.choices[0].message.content\n",
    "            # Всегда очищаем ответ от тегов <think>\n",
    "            cleaned_response = self._clean_response(raw_response_content)\n",
    "            return cleaned_response\n",
    "        except Exception as e:\n",
    "            print(f\"Error during synchronous generation for model '{self.model_name_original}', prompt '{prompt[:50]}...': {e}\")\n",
    "            # Добавляем подсказки, если ошибка связана с параметром 'enable_thinking'\n",
    "            if api_extra_params.get(\"enable_thinking\") and \"unexpected keyword argument 'enable_thinking'\" in str(e).lower():\n",
    "                 print(\"Hint: The API might not support 'enable_thinking' as a direct parameter. Try modifying the wrapper to use 'extra_body'.\")\n",
    "            elif api_extra_params.get(\"enable_thinking\") and \"extra_body\" in str(e).lower():\n",
    "                 print(\"Hint: Check the exact API documentation for SGLang/vLLM OpenAI-compatible endpoint for enabling Qwen3 thinking mode.\")\n",
    "            return \"\" # Возвращаем пустую строку в случае ошибки\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates a response asynchronously.\n",
    "        If attempt_thinking_mode is True, it modifies the request based on the model.\n",
    "        It always cleans the response to remove <think> tags.\n",
    "        \"\"\"\n",
    "        client = self.load_async_model()\n",
    "        # Подготавливаем аргументы для вызова API\n",
    "        call_args = self._prepare_api_call_args(prompt)\n",
    "        messages = call_args[\"messages\"]\n",
    "        api_extra_params = call_args[\"api_extra_params\"]\n",
    "\n",
    "        try:\n",
    "            # Выполняем асинхронный вызов API\n",
    "            response = await client.chat.completions.create(\n",
    "                model=self.model_name_original, # Используем оригинальное имя модели\n",
    "                messages=messages,\n",
    "                max_tokens=self.max_tokens_to_generate,\n",
    "                **api_extra_params # Распаковываем доп. параметры\n",
    "                 # Примечание: Если 'enable_thinking' не работает как прямой параметр,\n",
    "                 # попробуйте передать его так (закомментировав строку выше с **api_extra_params):\n",
    "                 # extra_body=api_extra_params if api_extra_params else None\n",
    "            )\n",
    "            # Получаем сырой текстовый ответ\n",
    "            raw_response_content = response.choices[0].message.content\n",
    "            # Всегда очищаем ответ от тегов <think>\n",
    "            cleaned_response = self._clean_response(raw_response_content)\n",
    "            return cleaned_response\n",
    "        except Exception as e:\n",
    "            print(f\"Error during asynchronous generation for model '{self.model_name_original}', prompt '{prompt[:50]}...': {e}\")\n",
    "            # Добавляем аналогичные подсказки про enable_thinking / extra_body\n",
    "            if api_extra_params.get(\"enable_thinking\") and \"unexpected keyword argument 'enable_thinking'\" in str(e).lower():\n",
    "                 print(\"Hint: The API might not support 'enable_thinking' as a direct parameter. Try modifying the wrapper to use 'extra_body'.\")\n",
    "            elif api_extra_params.get(\"enable_thinking\") and \"extra_body\" in str(e).lower():\n",
    "                 print(\"Hint: Check the exact API documentation for SGLang/vLLM OpenAI-compatible endpoint for enabling Qwen3 thinking mode.\")\n",
    "            return \"\" # Возвращаем пустую строку в случае ошибки\n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns the original name of the model used for initialization.\n",
    "        \"\"\"\n",
    "        # Возвращаем оригинальное имя, сохраненное при инициализации\n",
    "        return self.model_name_original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e8311c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CogitoLLM= SGlangModel(model_name=\"deepcogito/cogito-v1-preview-llama-8B\", base_url=\"http://85.143.167.11:30000/v1\")\n",
    "\n",
    "# Qwen3_30_Reasoning = LLMModel(model_name=\"qwen3-30-lmstudio\", base_url=\"http://85.143.167.11:30000/v1\", attempt_thinking_mode=True, cleaning_method=\"rsplit\")\n",
    "# Qwen3_30 = LLMModel(model_name=\"qwen3-30-lmstudio\", base_url=\"http://85.143.167.11:30000/v1\", attempt_thinking_mode=False)\n",
    "\n",
    "# Qwen3_32_Reasoning = LLMModel(model_name=\"Qwen/Qwen3-32B-AWQ\", base_url=\"http://85.143.167.11:30000/v1\", attempt_thinking_mode=True, cleaning_method=\"rsplit\")\n",
    "# Qwen3_32 = LLMModel(model_name=\"Qwen/Qwen3-32B-AWQ\", base_url=\"http://85.143.167.11:30000/v1\", attempt_thinking_mode=False)\n",
    "\n",
    "# Qwen3_8_Reasoning = LLMModel(model_name=\"Qwen/Qwen3-8B-FP8\", base_url=\"http://85.143.167.11:30000/v1\", attempt_thinking_mode=True, cleaning_method=\"rsplit\")\n",
    "# Qwen3_8 = LLMModel(model_name=\"Qwen/Qwen3-8B-FP8\", base_url=\"http://85.143.167.11:30000/v1\", attempt_thinking_mode=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd754b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "✨ 🚀 ✨ Loading Documents: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Database error: error returned from database: (code: 14) unable to open database file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m context_construction_config = ContextConstructionConfig(embedder = BertaEmbeddings,\n\u001b[32m      2\u001b[39m                                                         critic_model = CogitoLLM,\n\u001b[32m      3\u001b[39m                                                         context_quality_threshold = \u001b[32m0.1\u001b[39m) \n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m data = \u001b[43msynthesizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_goldens_from_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocument_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/home/user/Projects/data-extracting/data/Kapandzhi_-_Pozvonochnik-276-284.pdf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_construction_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_construction_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/synthesizer.py:129\u001b[39m, in \u001b[36mSynthesizer.generate_goldens_from_docs\u001b[39m\u001b[34m(self, document_paths, include_expected_output, max_goldens_per_context, context_construction_config, _send_data)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_mode:\n\u001b[32m    128\u001b[39m     loop = get_or_create_event_loop()\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     goldens = \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ma_generate_goldens_from_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdocument_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocument_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m            \u001b[49m\u001b[43minclude_expected_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_expected_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_goldens_per_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_goldens_per_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontext_construction_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_construction_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_reset_cost\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# Generate contexts from provided docs\u001b[39;00m\n\u001b[32m    140\u001b[39m     context_generator = ContextGenerator(\n\u001b[32m    141\u001b[39m         document_paths=document_paths,\n\u001b[32m    142\u001b[39m         encoding=context_construction_config.encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    149\u001b[39m         max_retries=context_construction_config.max_retries,\n\u001b[32m    150\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/asyncio/futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/asyncio/tasks.py:277\u001b[39m, in \u001b[36mTask.__step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    275\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    276\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    279\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/synthesizer.py:220\u001b[39m, in \u001b[36mSynthesizer.a_generate_goldens_from_docs\u001b[39m\u001b[34m(self, document_paths, include_expected_output, max_goldens_per_context, context_construction_config, _reset_cost)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# Generate contexts from provided docs\u001b[39;00m\n\u001b[32m    208\u001b[39m context_generator = ContextGenerator(\n\u001b[32m    209\u001b[39m     document_paths=document_paths,\n\u001b[32m    210\u001b[39m     encoding=context_construction_config.encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    217\u001b[39m     max_retries=context_construction_config.max_retries,\n\u001b[32m    218\u001b[39m )\n\u001b[32m    219\u001b[39m contexts, source_files, context_scores = (\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m context_generator.a_generate_contexts(\n\u001b[32m    221\u001b[39m         max_contexts_per_source_file=context_construction_config.max_contexts_per_document,\n\u001b[32m    222\u001b[39m         min_contexts_per_source_file=context_construction_config.min_contexts_per_document,\n\u001b[32m    223\u001b[39m         max_context_size=context_construction_config.max_context_length,\n\u001b[32m    224\u001b[39m         min_context_size=context_construction_config.min_context_length,\n\u001b[32m    225\u001b[39m     )\n\u001b[32m    226\u001b[39m )\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.synthesis_cost:\n\u001b[32m    228\u001b[39m     \u001b[38;5;28mself\u001b[39m.synthesis_cost += context_generator.total_cost\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:204\u001b[39m, in \u001b[36mContextGenerator.a_generate_contexts\u001b[39m\u001b[34m(self, max_contexts_per_source_file, min_contexts_per_source_file, max_context_size, min_context_size)\u001b[39m\n\u001b[32m    198\u001b[39m     source_files_to_chunk_collections_map[key] = collection\n\u001b[32m    200\u001b[39m tasks = [\n\u001b[32m    201\u001b[39m     a_chunk_and_store(key, chunker)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, chunker \u001b[38;5;129;01min\u001b[39;00m source_file_to_chunker_map.items()\n\u001b[32m    203\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m tqdm_asyncio.gather(\n\u001b[32m    205\u001b[39m     *tasks, desc=\u001b[33m\"\u001b[39m\u001b[33m✨ 📚 ✨ Chunking Documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    206\u001b[39m )\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# Intialize progress bar for context generation\u001b[39;00m\n\u001b[32m    209\u001b[39m num_contexts = \u001b[38;5;28msum\u001b[39m(\n\u001b[32m    210\u001b[39m     \u001b[38;5;28mmin\u001b[39m(max_contexts_per_source_file, collection.count())\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, collection \u001b[38;5;129;01min\u001b[39;00m source_files_to_chunk_collections_map.items()\n\u001b[32m    212\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/tqdm/asyncio.py:79\u001b[39m, in \u001b[36mtqdm_asyncio.gather\u001b[39m\u001b[34m(cls, loop, timeout, total, *fs, **tqdm_kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n\u001b[32m     78\u001b[39m ifs = [wrap_awaitable(i, f) \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fs)]\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m res = [\u001b[38;5;28;01mawait\u001b[39;00m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.as_completed(ifs, loop=loop, timeout=timeout,\n\u001b[32m     80\u001b[39m                                          total=total, **tqdm_kwargs)]\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m _, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(res)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/tqdm/asyncio.py:79\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n\u001b[32m     78\u001b[39m ifs = [wrap_awaitable(i, f) \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fs)]\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m res = [\u001b[38;5;28;01mawait\u001b[39;00m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.as_completed(ifs, loop=loop, timeout=timeout,\n\u001b[32m     80\u001b[39m                                          total=total, **tqdm_kwargs)]\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m _, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(res)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/asyncio/tasks.py:615\u001b[39m, in \u001b[36mas_completed.<locals>._wait_for_one\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    613\u001b[39m     \u001b[38;5;66;03m# Dummy value from _on_timeout().\u001b[39;00m\n\u001b[32m    614\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.TimeoutError\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/asyncio/futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/asyncio/tasks.py:277\u001b[39m, in \u001b[36mTask.__step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    275\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    276\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    279\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/tqdm/asyncio.py:76\u001b[39m, in \u001b[36mtqdm_asyncio.gather.<locals>.wrap_awaitable\u001b[39m\u001b[34m(i, f)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_awaitable\u001b[39m(i, f):\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:192\u001b[39m, in \u001b[36mContextGenerator.a_generate_contexts.<locals>.a_chunk_and_store\u001b[39m\u001b[34m(key, chunker)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34ma_chunk_and_store\u001b[39m(key, chunker: DocumentChunker):\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     collection = \u001b[38;5;28;01mawait\u001b[39;00m chunker.a_chunk_doc(\n\u001b[32m    193\u001b[39m         \u001b[38;5;28mself\u001b[39m.chunk_size, \u001b[38;5;28mself\u001b[39m.chunk_overlap\n\u001b[32m    194\u001b[39m     )\n\u001b[32m    195\u001b[39m     \u001b[38;5;28mself\u001b[39m.validate_chunk_size(\n\u001b[32m    196\u001b[39m         min_contexts_per_source_file, collection\n\u001b[32m    197\u001b[39m     )\n\u001b[32m    198\u001b[39m     source_files_to_chunk_collections_map[key] = collection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/doc_chunker.py:57\u001b[39m, in \u001b[36mDocumentChunker.a_chunk_doc\u001b[39m\u001b[34m(self, chunk_size, chunk_overlap)\u001b[39m\n\u001b[32m     55\u001b[39m full_document_path, _ = os.path.splitext(\u001b[38;5;28mself\u001b[39m.source_file)\n\u001b[32m     56\u001b[39m document_name = os.path.basename(full_document_path)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m client = \u001b[43mchromadb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPersistentClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.vector_db/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdocument_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m collection_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprocessed_chunks_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_overlap\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/chromadb/__init__.py:161\u001b[39m, in \u001b[36mPersistentClient\u001b[39m\u001b[34m(path, settings, tenant, database)\u001b[39m\n\u001b[32m    158\u001b[39m tenant = \u001b[38;5;28mstr\u001b[39m(tenant)\n\u001b[32m    159\u001b[39m database = \u001b[38;5;28mstr\u001b[39m(database)\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mClientCreator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m=\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/chromadb/api/client.py:85\u001b[39m, in \u001b[36mClient.__init__\u001b[39m\u001b[34m(self, tenant, database, settings)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Create an admin client for verifying that databases and tenants exist\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mself\u001b[39m._admin_client = AdminClient.from_system(\u001b[38;5;28mself\u001b[39m._system)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_tenant_database\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m._submit_client_start_event()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/chromadb/api/client.py:433\u001b[39m, in \u001b[36mClient._validate_tenant_database\u001b[39m\u001b[34m(self, tenant, database)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;66;03m# Propagate ChromaErrors\u001b[39;00m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ChromaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    436\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not connect to tenant \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtenant\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Are you sure it exists?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    437\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/chromadb/api/client.py:426\u001b[39m, in \u001b[36mClient._validate_tenant_database\u001b[39m\u001b[34m(self, tenant, database)\u001b[39m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_tenant_database\u001b[39m(\u001b[38;5;28mself\u001b[39m, tenant: \u001b[38;5;28mstr\u001b[39m, database: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    425\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_admin_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tenant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    427\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m    428\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    429\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCould not connect to a Chroma server. Are you sure it is running?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    430\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/chromadb/api/client.py:483\u001b[39m, in \u001b[36mAdminClient.get_tenant\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    481\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_tenant\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Tenant:\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_server\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tenant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-AMH9POEK-py3.11/lib/python3.11/site-packages/chromadb/api/rust.py:166\u001b[39m, in \u001b[36mRustBindingsAPI.get_tenant\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_tenant\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Tenant:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m     tenant = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tenant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Tenant(name=tenant.name)\n",
      "\u001b[31mInternalError\u001b[39m: Database error: error returned from database: (code: 14) unable to open database file"
     ]
    }
   ],
   "source": [
    "context_construction_config = ContextConstructionConfig(embedder = BertaEmbeddings,\n",
    "                                                        critic_model = CogitoLLM,\n",
    "                                                        context_quality_threshold = 0.1) \n",
    "\n",
    "data = synthesizer.generate_goldens_from_docs(\n",
    "    document_paths=['/home/user/Projects/data-extracting/data/Kapandzhi_-_Pozvonochnik-276-284.pdf'],\n",
    "    context_construction_config=context_construction_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79b80e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.synthesizer import Synthesizer, Evolution  \n",
    "from deepeval.synthesizer.config import StylingConfig, EvolutionConfig, ContextConstructionConfig, FiltrationConfig  \n",
    "  \n",
    "context_construction_config = ContextConstructionConfig(  \n",
    "    chunk_size=2048,                 \n",
    "    chunk_overlap=0,            \n",
    "    max_contexts_per_document=3,\n",
    "    context_quality_threshold=0.4,\n",
    "    max_retries=2,\n",
    "    critic_model = CogitoLLM,\n",
    "    embedder = BertaEmbeddings)  \n",
    "  \n",
    "styling_config = StylingConfig(  \n",
    "    input_format=\"\"\"Сложные академические вопросы на русском языке, основанные на содержании медицинских учебников.   \n",
    "    Вопросы должны быть сформулированы с использованием точной медицинской терминологии,   \n",
    "    требовать глубокого понимания материала и проверять способность применять теоретические знания   \n",
    "    к клиническим ситуациям.\"\"\",  \n",
    "      \n",
    "    expected_output_format=\"\"\"Структурированные, научно обоснованные ответы на русском языке,   \n",
    "    включающие ключевые концепции из учебника, точные определения, классификации и механизмы.   \n",
    "    Ответы должны быть организованы в логической последовательности с использованием подзаголовков, где это уместно.\"\"\",  \n",
    "      \n",
    "    task=\"\"\"Создание высококачественных экзаменационных вопросов на русском языке,   \n",
    "    которые точно отражают содержание учебника и проверяют глубину понимания материала.\"\"\",  \n",
    "      \n",
    "    scenario=\"\"\"Преподаватель медицинского вуза создает банк вопросов для экзаменов,   \n",
    "    тестирования и самопроверки студентов. Вопросы должны точно соответствовать   \n",
    "    содержанию учебников и быть пригодными для оценки компетенций студентов   \n",
    "    разных курсов медицинского образования.\"\"\",  \n",
    ")  \n",
    "  \n",
    "evolution_config = EvolutionConfig(  \n",
    "    num_evolutions=2, \n",
    "    evolutions={  \n",
    "        Evolution.CONCRETIZING: 0.25,     \n",
    "        Evolution.MULTICONTEXT: 0.25,   \n",
    "        Evolution.COMPARATIVE: 0.25,   \n",
    "        Evolution.CONSTRAINED: 0.25,\n",
    "    }  \n",
    ") \n",
    "\n",
    "filtration_config = FiltrationConfig(  \n",
    "    synthetic_input_quality_threshold=0.5,\n",
    "    critic_model= CogitoLLM,\n",
    "    max_quality_retries=2                \n",
    ")\n",
    "\n",
    "synthesizer = Synthesizer(\n",
    "    model=CogitoLLM,  \n",
    "    styling_config=styling_config,  \n",
    "    # evolution_config=evolution_config,\n",
    "    # filtration_config=filtration_config,  \n",
    "    async_mode=True,  \n",
    "    max_concurrent=8\n",
    ")  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f92b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "✨ 🚀 ✨ Loading Documents: 100%|██████████| 2/2 [00:02<00:00,  1.00s/it]\n",
      "✨ 📚 ✨ Chunking Documents: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]\n",
      "✨ 🧩 ✨ Generating Contexts:   0%|          | 0/12 [00:00<?, ?it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Evaluation LLM outputted an invalid JSON. Please use a better evaluation model.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/metrics/utils.py:274\u001b[39m, in \u001b[36mtrimAndLoadJson\u001b[39m\u001b[34m(input_string, metric)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjsonStr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m json.JSONDecodeError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/json/decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m end = _w(s, end).end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.10/lib/python3.11/json/decoder.py:355\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m goldens = \u001b[38;5;28;01mawait\u001b[39;00m synthesizer.a_generate_goldens_from_docs(  \n\u001b[32m      2\u001b[39m     document_paths=[\u001b[33m'\u001b[39m\u001b[33m/mnt/sdb1/PycharmProjects/CODUP/AI-tutor-other/docs/for_golds/Anatomia_cheloveka_1_tom_2-52-57.pdf\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      3\u001b[39m                     \u001b[33m'\u001b[39m\u001b[33m/mnt/sdb1/PycharmProjects/CODUP/AI-tutor-other/docs/for_golds/Kapandzhi_-_Pozvonochnik-276-284.pdf\u001b[39m\u001b[33m'\u001b[39m],  \n\u001b[32m      4\u001b[39m     include_expected_output=\u001b[38;5;28;01mTrue\u001b[39;00m,  \n\u001b[32m      5\u001b[39m     max_goldens_per_context=\u001b[32m3\u001b[39m,  \n\u001b[32m      6\u001b[39m     context_construction_config=context_construction_config  \n\u001b[32m      7\u001b[39m )  \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/synthesizer.py:220\u001b[39m, in \u001b[36mSynthesizer.a_generate_goldens_from_docs\u001b[39m\u001b[34m(self, document_paths, include_expected_output, max_goldens_per_context, context_construction_config, _reset_cost)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# Generate contexts from provided docs\u001b[39;00m\n\u001b[32m    208\u001b[39m context_generator = ContextGenerator(\n\u001b[32m    209\u001b[39m     document_paths=document_paths,\n\u001b[32m    210\u001b[39m     encoding=context_construction_config.encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    217\u001b[39m     max_retries=context_construction_config.max_retries,\n\u001b[32m    218\u001b[39m )\n\u001b[32m    219\u001b[39m contexts, source_files, context_scores = (\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m context_generator.a_generate_contexts(\n\u001b[32m    221\u001b[39m         max_contexts_per_source_file=context_construction_config.max_contexts_per_document,\n\u001b[32m    222\u001b[39m         min_contexts_per_source_file=context_construction_config.min_contexts_per_document,\n\u001b[32m    223\u001b[39m         max_context_size=context_construction_config.max_context_length,\n\u001b[32m    224\u001b[39m         min_context_size=context_construction_config.min_context_length,\n\u001b[32m    225\u001b[39m     )\n\u001b[32m    226\u001b[39m )\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.synthesis_cost:\n\u001b[32m    228\u001b[39m     \u001b[38;5;28mself\u001b[39m.synthesis_cost += context_generator.total_cost\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:241\u001b[39m, in \u001b[36mContextGenerator.a_generate_contexts\u001b[39m\u001b[34m(self, max_contexts_per_source_file, min_contexts_per_source_file, max_context_size, min_context_size)\u001b[39m\n\u001b[32m    230\u001b[39m     max_context_size = \u001b[38;5;28mmin\u001b[39m(max_context_size, collection.count())\n\u001b[32m    231\u001b[39m     tasks.append(\n\u001b[32m    232\u001b[39m         \u001b[38;5;28mself\u001b[39m._a_process_document_async(\n\u001b[32m    233\u001b[39m             path,\n\u001b[32m   (...)\u001b[39m\u001b[32m    238\u001b[39m         )\n\u001b[32m    239\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path, contexts_per_doc, scores_per_doc \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[32m    243\u001b[39m     contexts.extend(contexts_per_doc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:269\u001b[39m, in \u001b[36mContextGenerator._a_process_document_async\u001b[39m\u001b[34m(self, path, num_context_per_source_file, max_context_size, generation_p_bar, source_files_to_collections_map)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_a_process_document_async\u001b[39m(\n\u001b[32m    261\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    262\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    266\u001b[39m     source_files_to_collections_map: Dict,\n\u001b[32m    267\u001b[39m ):\n\u001b[32m    268\u001b[39m     contexts_per_doc, scores_per_doc = (\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._a_get_n_random_contexts_per_source_file(\n\u001b[32m    270\u001b[39m             path=path,\n\u001b[32m    271\u001b[39m             n_contexts_per_source_file=num_context_per_source_file,\n\u001b[32m    272\u001b[39m             context_size=max_context_size,\n\u001b[32m    273\u001b[39m             similarity_threshold=\u001b[38;5;28mself\u001b[39m.similarity_threshold,\n\u001b[32m    274\u001b[39m             generation_p_bar=generation_p_bar,\n\u001b[32m    275\u001b[39m             source_files_to_collections_map=source_files_to_collections_map,\n\u001b[32m    276\u001b[39m         )\n\u001b[32m    277\u001b[39m     )\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m path, contexts_per_doc, scores_per_doc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:385\u001b[39m, in \u001b[36mContextGenerator._a_get_n_random_contexts_per_source_file\u001b[39m\u001b[34m(self, path, n_contexts_per_source_file, context_size, similarity_threshold, generation_p_bar, source_files_to_collections_map)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# Sample n random chunks from each doc (each random chunk is the first chunk in each context)\u001b[39;00m\n\u001b[32m    379\u001b[39m filling_p_bar = tqdm_bar(\n\u001b[32m    380\u001b[39m     total=(context_size - \u001b[32m1\u001b[39m) * n_contexts_per_source_file,\n\u001b[32m    381\u001b[39m     desc=\u001b[33m\"\u001b[39m\u001b[33m  ✨ 🫗 ✨ Filling Contexts\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    382\u001b[39m     leave=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m )\n\u001b[32m    384\u001b[39m random_chunks, scores = (\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._a_get_n_random_chunks_per_source_file(\n\u001b[32m    386\u001b[39m         path,\n\u001b[32m    387\u001b[39m         n_contexts_per_source_file,\n\u001b[32m    388\u001b[39m         generation_p_bar,\n\u001b[32m    389\u001b[39m         source_files_to_collections_map,\n\u001b[32m    390\u001b[39m     )\n\u001b[32m    391\u001b[39m )\n\u001b[32m    393\u001b[39m \u001b[38;5;66;03m# Find similar chunks for each context\u001b[39;00m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m random_chunk \u001b[38;5;129;01min\u001b[39;00m random_chunks:\n\u001b[32m    395\u001b[39m     \u001b[38;5;66;03m# Create context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:528\u001b[39m, in \u001b[36mContextGenerator._a_get_n_random_chunks_per_source_file\u001b[39m\u001b[34m(self, path, n_chunks, p_bar, source_files_to_collections_map)\u001b[39m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m score\n\u001b[32m    527\u001b[39m tasks = [a_evaluate_chunk_and_update(chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m scores = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m    529\u001b[39m chunk_score_pairs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(chunks, scores))\n\u001b[32m    530\u001b[39m chunk_score_pairs.sort(key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m1\u001b[39m], reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:523\u001b[39m, in \u001b[36mContextGenerator._a_get_n_random_chunks_per_source_file.<locals>.a_evaluate_chunk_and_update\u001b[39m\u001b[34m(chunk)\u001b[39m\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34ma_evaluate_chunk_and_update\u001b[39m(chunk):\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m     score = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.a_evaluate_chunk(chunk)\n\u001b[32m    524\u001b[39m     p_bar.update(\u001b[32m1\u001b[39m)\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/synthesizer/chunking/context_generator.py:582\u001b[39m, in \u001b[36mContextGenerator.a_evaluate_chunk\u001b[39m\u001b[34m(self, chunk)\u001b[39m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    581\u001b[39m     res: ContextScore = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.a_generate(prompt)\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m     data = \u001b[43mtrimAndLoadJson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m     score = (\n\u001b[32m    584\u001b[39m         data[\u001b[33m\"\u001b[39m\u001b[33mclarity\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    585\u001b[39m         + data[\u001b[33m\"\u001b[39m\u001b[33mdepth\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    586\u001b[39m         + data[\u001b[33m\"\u001b[39m\u001b[33mstructure\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    587\u001b[39m         + data[\u001b[33m\"\u001b[39m\u001b[33mrelevance\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    588\u001b[39m     ) / \u001b[32m4\u001b[39m\n\u001b[32m    589\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/diploma-au9TFMuk-py3.11/lib/python3.11/site-packages/deepeval/metrics/utils.py:279\u001b[39m, in \u001b[36mtrimAndLoadJson\u001b[39m\u001b[34m(input_string, metric)\u001b[39m\n\u001b[32m    277\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    278\u001b[39m         metric.error = error_str\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_str)\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAn unexpected error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Evaluation LLM outputted an invalid JSON. Please use a better evaluation model."
     ]
    }
   ],
   "source": [
    "goldens = await synthesizer.a_generate_goldens_from_docs(  \n",
    "    document_paths=['/mnt/sdb1/PycharmProjects/CODUP/AI-tutor-other/docs/for_golds/Anatomia_cheloveka_1_tom_2-52-57.pdf',\n",
    "                    '/mnt/sdb1/PycharmProjects/CODUP/AI-tutor-other/docs/for_golds/Kapandzhi_-_Pozvonochnik-276-284.pdf'],  \n",
    "    include_expected_output=True,  \n",
    "    max_goldens_per_context=3,  \n",
    "    context_construction_config=context_construction_config  \n",
    ")  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f94c79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83051093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deepeval.dataset import EvaluationDataset  \n",
    "# dataset = EvaluationDataset(goldens=goldens)  \n",
    "# dataset.push(alias=\"Медицинские вопросы из учебников\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diploma-au9TFMuk-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
